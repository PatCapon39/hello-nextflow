{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello Nextflow workshop","text":"<p>This workshop will provide you with the foundational knowledge required to build Nextflow workflows. The content is broken up into 2 parts. In the first part we will cover the basic principles for developing Nextflow pipelines. In the second part we will step through building our own Nextflow workflow. See the lesson plan for details.</p>"},{"location":"#trainers","title":"Trainers","text":"<ul> <li>Fred Jaya, Sydney Informatics Hub, University of Sydney</li> <li>Georgie Samaha, Sydney Informatics Hub, University of Sydney</li> <li>Chris Hakkaart, Seqera</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This is an intermediate-advanced workshop for people developing reproducible bioinformatics workflows.</p> <ul> <li>Experience working on the command line/Linux environment.</li> <li>Experience developing reproducible workflows (e.g., bash, CWL, WDL, or Snakemake). </li> </ul>"},{"location":"#set-up-requirements","title":"Set up requirements","text":"<p>Please complete the Setup instruction before the workshop. If you have any trouble, please get in contact with us ASAP via Slack.</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>In order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:</p> <ul> <li>Use welcoming and inclusive language</li> <li>Be respectful of different viewpoints and experiences</li> <li>Gracefully accept constructive criticism</li> <li>Focus on what is best for the community</li> <li>Show courtesy and respect towards other community members</li> <li>Our full code of conduct, with incident reporting guidelines, is available here.</li> </ul>"},{"location":"#workshop-schedule","title":"Workshop schedule","text":""},{"location":"#day-1","title":"Day 1","text":"Time (AEST) Activity Presenter(s) 14:00 Welcome Melissa 14:10 Introduction to the workshop Georgie 14:15 Workspace set up\u00a0 Georgie 14:20 Introduction to Nextflow and nf-core Chris 14:30 Workshop: Part 1.0 - 1.4 Chris 15:30 BREAK Melissa 15:45 Workshop: Part 1.5 - 1.8 Chris 16:45 Q&amp;A and Day 1 recap Chris, Melissa"},{"location":"#day-2","title":"Day 2","text":"Time (AEST) Activity Presenter(s) 14:00 Day 2 Welcome Melissa 14:05 Setup and Introduction to Part 2 Fred 14:15 Workshop: Part 2.1 - 2.3 Fred 15:30 BREAK Melissa 15:45 Workshop: Part 2.4 - 2.5 Fred 16:40 Day 2 Summary Georgie 16:50 Wrap up and survey Melissa"},{"location":"#course-survey","title":"Course survey","text":"<p>Please fill out our course survey at the end of the workshop. Help us help you! \ud83d\ude01</p>"},{"location":"#credits-and-acknowledgements","title":"Credits and acknowledgements","text":"<p>This workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with Seqera. The workshop was enabled through the Australian BioCommons - BioCLI Platforms Project (NCRIS via Bioplatforms Australia). </p> <p></p>"},{"location":"resources/","title":"Supporting materials","text":""},{"location":"resources/#recommended-resources","title":"Recommended resources","text":"<p>Here are some useful resources we recommend to help you get started with running nf-core pipelines and developing Nextflow pipelines:</p>"},{"location":"resources/#developed-by-us","title":"Developed by us","text":"<ul> <li>SIH Nextflow template</li> <li>SIH Nextflow template guide</li> <li>SIH Customising nf-core workshop</li> <li>Australian BioCommons Seqera Platform Service</li> <li>NCI Gadi nf-core instutitonal config</li> <li>Pawsey Setonix nf-core instutitional config</li> </ul>"},{"location":"resources/#developed-by-others","title":"Developed by others","text":"<ul> <li>Nextflow training</li> <li>Nextflow patterns</li> <li>Nextflow blog</li> <li>Nextflow coding best practice recommendations</li> <li>Seqera community forums</li> </ul>"},{"location":"resources/#nextflow-tips-and-tricks","title":"Nextflow tips and tricks","text":"<p>Nextflow has some useful features for executing pipelines and querying metadata and history. Here are some resources to help you get started.</p>"},{"location":"resources/#query-specific-pipeline-executions","title":"Query specific pipeline executions","text":"<p>The Nextflow log command is useful for querying execution metadata and history. You can filter your queries and output specific fields in the printed log. </p> <pre><code>nextflow log &lt;run_name&gt; -help\n</code></pre>"},{"location":"resources/#execute-nextflow-in-the-background","title":"Execute Nextflow in the background","text":"<p>The <code>-bg</code> options allows you to run your pipeline in the background and continue using your terminal. It is similar to <code>nohup</code>. You can redirect all standard output to a log file. </p> <pre><code>nextflow run &lt;workflow_repo/main.nf&gt; -bg &gt; workshop_tip.log\n</code></pre>"},{"location":"resources/#capture-a-nextflow-pipelines-configuration","title":"Capture a Nextflow pipeline's configuration","text":"<p>The Nextflow config command prints the resolved pipeline configuration. It is especially useful for printing all resolved parameters and profiles Nextflow will use to run a pipeline. </p> <pre><code>nextflow config &lt;workflow_repo&gt; -help\n</code></pre>"},{"location":"resources/#clean-nextflow-cache-and-work-directories","title":"Clean Nextflow cache and work directories","text":"<p>The Nextflow clean command will remove files from previous executions stored in the <code>.nextflow</code> cache and <code>work</code> directories. The <code>-dry-run</code> option allows you to preview which files will be deleted. </p> <pre><code>nextflow clean &lt;workflow_repo&gt; -help\n</code></pre>"},{"location":"resources/#change-default-nextflow-cache-strategy","title":"Change default Nextflow cache strategy","text":"<p>Workflow execution is sometimes not resumed as expected. The default behaviour of Nextflow cache keys is to index the input files meta-data information. Reducing the cache stringency to <code>lenient</code> means the files cache keys are based only on filesize and path, and can help to avoid unexpectedly re-running certain processes when <code>-resume</code> is in use. </p> <p>To apply lenient cache strategy to all of your runs, you could add to a custom configuration file:</p> <pre><code>process {\n    cache = 'lenient'\n}\n</code></pre> <p>You can specify different cache stategies for different processes by using <code>withName</code> or <code>withLabel</code>. You can specify a particular cache strategy be applied to certain <code>profiles</code> within your institutional config, or to apply to all profiles described within that config by placing the above <code>process</code> code block outside the <code>profiles</code> scope.    </p>"},{"location":"resources/#access-private-github-repositories","title":"Access private GitHub repositories","text":"<p>To interact with private repositories on GitHub, you can provide Nextflow with access to GitHub by specifying your GitHub user name and a Personal Access Token in the <code>scm</code> configuration file inside your specified <code>.nextflow/</code> directory:</p> <pre><code>providers {\n\n  github {\n    user = 'georgiesamaha'\n    password = 'my-personal-access-token'\n  }\n\n}\n</code></pre>"},{"location":"setup/","title":"Setting up your computer","text":"<p>In this workshop, we will be using Virtual Machines (VM) on the ARDC Nectar Research Cloud.</p> <p>The requirements for this workshop are a personal computer with:</p> <ul> <li>Visual Studio Code (VSCode)</li> <li>A web browser</li> </ul> <p>Below, you will find instructions on how to set up VSCode and connect toyour VM. Each participant will be provided with their instances IP address prior to the workshop. Before the workshop, you must have the following:</p> <ol> <li>VSCode installed.</li> <li>The necessary VSCode extensions installed.</li> <li>Be able to connect to your VM.  </li> </ol> <p>Info</p> <p>If you require assistance with the setup, please message the Slack channel!</p>"},{"location":"setup/#installing-visual-studio-code","title":"Installing Visual Studio Code","text":"<p>Visual Studio Code (VSCode) is a versatile code editor that we will use for the workshop. We will use VSCode to connect to the VM, navigate the directories, edit, view and download files. </p> <ol> <li>Download VSCode by following the  installation instructions for your local Operating System.  </li> <li>Open VSCode to confirm it was installed correctly.  </li> </ol> <p></p>"},{"location":"setup/#installing-the-vscode-extensions","title":"Installing the VSCode extensions","text":"<p>Specific VSCode extensions are required to connect to the VM and make working with Nextflow files easier (i.e. syntax highlighting).  </p> <ol> <li>In the VSCode sidebar on the left, click on the extensions button (four blocks).</li> <li>In the Extensions Marketplace search bar, search for <code>remote ssh</code>. Select \"Remote - SSH\".</li> </ol> <p> 3. Click on the blue <code>Install</code> button.</p> <p> 4. Search for <code>nextflow</code> and install the \"Nextflow\" extension.  </p> <p> 5. Close the Extensions tab and sidebar</p>"},{"location":"setup/#setting-up-your-remote-ssh-config","title":"Setting up your remote SSH config","text":"<ol> <li>In VSCode, press <code>Ctrl+Shift+P</code> (<code>Command+Shift+P</code> on mac) to open the Command Palette.  </li> </ol> <p> 2. Type <code>remote ssh</code> and select <code>Remote-SSH: Add New SSH Host...</code>. This may appear in a different position in the list.</p> <p> 3. Enter the SSH connection string with the IP address that was provided to you. The connection string should look like <code>ssh user1@XXX.XXX.XX.XX</code>. Ensure that you replace the \"XXX...\" with your allocated IP address. Press <code>Enter</code>.</p> <p> 4. You will be prompted to <code>Select SSH configuration file to update</code>. Select your <code>.ssh/config</code> file. </p> <p> 5. You should receive a pop-up informing that a host as been added!</p>"},{"location":"setup/#connecting-to-the-vm","title":"Connecting to the VM","text":"<p>Ensure you have configured your SSH details.  </p> <ol> <li> <p>In VSCode, press <code>Ctrl+Shift+P</code> (<code>Command+Shift+P</code> on mac) to open the Command Palette.  </p> </li> <li> <p>Type <code>remote ssh</code> and select <code>Remote-SSH: Connect to Host...</code>. This may appear in a different position in the list.</p> </li> </ol> <p> 3. Select the IP address that you have configured. If you are prompted for a platform, select \"Linux\". 4. A new VSCode window will open and prompt you for your password. Input your allocated password and hit 'Enter'. </p> <p> 5. In the pop-up, Select \"Yes, I trust the authors\" 6. Once the blue square in the bottom-left of the VSCode window shows  <code>SSH: XXX.XXX.XX.XX</code> - you have successfully connected to your instance!  </p> <p> </p>"},{"location":"setup/#configuring-vscode-for-the-workshop","title":"Configuring VSCode for the workshop","text":"<ol> <li>Select the File Explorer on the left sidebar (icon with two pages) or press  <code>Ctrl+Shift+E</code> (Mac: <code>Cmd+Shift+E</code>).  </li> </ol> <p> 2. Select <code>Open Folder</code></p> <p> 3. The correct file path should be input by default (<code>/home/userX/</code>). Press 'OK'. 4. The home directory will appear in the left side bar.  </p> <p> 6. In the Explorer sidebar, select <code>.main.nf</code>. This file will open in a tab. You may need to re-enter you password again.  7. Check that syntax highlighting (different parts of the Nextflow code are  coloured) is enabled as shown. This is to confirm the VSCode Nextflow extension is working correctly.  </p> <p> </p> <p>Warning</p> <p>The <code>.main.nf</code> file is for testing purposes only. We will not touch this file in the workshop.</p> <p>8. Toggle the terminal in VSCode by pressing `Ctrl+j` (`Cmd+j` on mac).</p> <p> </p> <p>Success</p> <p>You have now configured VSCode for the workshop! </p>"},{"location":"part1/00_intro/","title":"Welcome","text":"<p>In Part 1 of this workshop, you will create a toy multi-step Nextflow workflow.</p> <p>We will start Part 1 by familiarizing ourselves with some common bash commands. Next, we will turn these commands into a small single-step Nextflow pipeline that will print a greeting to our terminal. In a series of exercises, we will then iterate on this pipeline to make it more flexible using variable outputs, inputs, and parameters. Finally, we will add a second step to the pipeline to dynamically turn our greeting into uppercase letters and name pipeline outputs.</p> <p>During Part 2, the skills and concepts you have learned in Part 1 will be applied in a more realistic scenario.</p>"},{"location":"part1/00_intro/#moving-into-your-work-directory","title":"Moving into your work directory","text":"<p>It is good practice to organize projects into their own folders to make it easier to track and replicate experiments over time. We have created separate directories for each part (<code>~/part1/</code> and <code>~/part2/</code>).</p> <p>Exercise</p> <p>In the VSCode terminal, move into the directory for all Part 1 activities:</p> <pre><code>cd ~/part1\n</code></pre>"},{"location":"part1/01_hellonextflow/","title":"Why Nextflow?","text":"<p>Learning objectives</p> <ol> <li>Describe the benefits of using Nextflow to write complex pipelines</li> <li>List community resources you can access to get help developing your pipelines</li> </ol> <p>Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.</p> <p>It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.</p> <p>Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model.</p>"},{"location":"part1/01_hellonextflow/#core-features","title":"Core features","text":"<p>Nextflow\u2019s core features are:</p> <ul> <li>Workflow portability and reproducibility</li> <li>Scalability of parallelization and deployment</li> <li>Integration of existing tools, systems, and industry standards</li> </ul>"},{"location":"part1/01_hellonextflow/#processes-tasks-and-channels","title":"Processes, tasks, and channels","text":"<p>A Nextflow workflow is made by joining together processes. Each process can be written in any scripting language that can be executed by the Linux platform Processes can be written in any language that can be executed from the command line, such as Bash, Python, or R.</p> <p>Processes in are executed independently (i.e., they do not share a common writable state) as tasks and can run in parallel, allowing for efficient utilization of computing resources. Nextflow automatically manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.</p> <p>The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called channels. Simply, every input and output of a process is represented as a channel. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.</p> <p></p>"},{"location":"part1/01_hellonextflow/#execution-abstraction","title":"Execution abstraction","text":"<p>While a process defines what command or script is executed, the executor determines how and where the script is executed.</p> <p>Nextflow provides an abstraction between the workflow\u2019s functional logic and the underlying execution system. This abstraction allows users to define a workflow once and execute it on different computing platforms without having to modify the workflow definition. Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments.</p> <p></p>"},{"location":"part1/01_hellonextflow/#more-information","title":"More information","text":"<p>This workshop focuses on basic skills for developers.</p> <p>Follow these links to find out more about Nextflow:</p> <ul> <li>Nextflow docs</li> <li>nf-core</li> <li>Nextflow training</li> <li>Reproducible workflows with nf-core</li> <li>Seqera community</li> </ul> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to describe different parts of a Nextflow pipeline</li> <li>How to find more information</li> </ol>"},{"location":"part1/02_helloworld/","title":"Hello World!","text":"<p>Learning objectives</p> <ol> <li>Utilize simple bash commands to manipulate strings</li> </ol> <p>A Hello, World! is a minimalist example that is meant to demonstrate the basic syntax and structure of a programming language or software framework. The example typically consists of printing the phrase 'Hello World!' to the output, such as the console or terminal, or writing it to a file.</p> <p>Let's demonstrate this with simple commands that you can run directly in the terminal.</p>"},{"location":"part1/02_helloworld/#print-a-string","title":"Print a string","text":"<p>The <code>echo</code> command in Linux is a built-in command that allows users to display lines of text or strings that are passed as arguments. It is commonly used in shell scripts and batch files to output status text to the screen or a file.</p> <p>The most straightforward usage of the <code>echo</code> command is to display a text or string on the terminal. To do this, you simply provide the desired text or string as an argument to the <code>echo</code> command:</p> <pre><code>echo &lt;string&gt;\n</code></pre> <p>Exercise</p> <p>Use the <code>echo</code> command to print the string <code>'Hello World!'</code> to the terminal.</p> Solution <pre><code>echo 'Hello World!'\n</code></pre>"},{"location":"part1/02_helloworld/#redirect-outputs","title":"Redirect outputs","text":"<p>The output of the <code>echo</code> can be redirected to a file instead of displaying it on the terminal. You can achieve this by using the <code>&gt;</code> operator for output redirection. For example:</p> <pre><code>echo 'Welcome!' &gt; output.txt\n</code></pre> <p>This will write the output of the echo command to the file name <code>output.txt</code>.</p> <p>Exercise</p> <p>Use the <code>&gt;</code> operator to redirect the output of echo to a file named <code>output.txt</code>.</p> Solution <pre><code>echo 'Hello World!' &gt; output.txt\n</code></pre>"},{"location":"part1/02_helloworld/#list-files","title":"List files","text":"<p>The Linux shell command <code>ls</code> lists directory contents of files and directories. It provides valuable information about files, directories, and their attributes.</p> <p><code>ls</code> will display the contents of the current directory:</p> <pre><code>ls\n</code></pre> <p>Exercise</p> <p>List the files in the working directory to verify <code>output.txt</code> was created.</p> Solution <pre><code>ls\n</code></pre> <p>A file named <code>output.txt</code> should now be listed in your current directory.</p>"},{"location":"part1/02_helloworld/#view-file-contents","title":"View file contents","text":"<p>The <code>cat</code> command in Linux is a versatile companion for various file-related operations, allowing users to view, concatenate, create, copy, merge, and manipulate file contents.</p> <p>The most basic use of <code>cat</code> is to display the contents of a file on the terminal. This can be achieved by simply providing the filename as an argument:</p> <pre><code>cat &lt;file name&gt;\n</code></pre> <p>Exercise</p> <p>Use the <code>cat</code> command to print the contents of <code>output.txt</code>.</p> Solution <pre><code>cat output.txt\n</code></pre> <p>You should see <code>Hello World!</code> printed to your terminal.</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use the <code>echo</code> command to print a string to the terminal</li> <li>How to use the <code>&gt;</code> operator to redirect the output of <code>echo</code></li> <li>How to use the <code>ls</code> command to list the files in your working directory</li> <li>How to use the <code>cat</code> command to print the contents of files</li> </ol>"},{"location":"part1/03_hellonf/","title":"Your first pipeline","text":"<p>Learning objectives</p> <ol> <li>Write your first  Nextflow pipeline</li> <li>Execute your first Nextflow pipeline and understand the outputs</li> </ol> <p>Workflow languages are better than Bash scripts because they handle errors and run tasks in parallel more easily, which is important for complex jobs. They also have clearer structure, making it easier to maintain and work on with others.</p> <p>Here, you're going learn more about the Nextflow language and take your first steps making a your first pipeline with Nextflow.</p>"},{"location":"part1/03_hellonf/#hello-worldnf","title":"<code>hello-world.nf</code>","text":"<p>Nextflow pipelines need to be saved as <code>.nf</code> files.</p> <p>The process definition starts with the keyword <code>process</code>, followed by process name, and finally the process body delimited by curly braces. The process body must contain a <code>script</code> block which represents the command or, more generally, a script that is executed by it.</p> <p>A process may contain any of the following definition blocks: <code>directives</code>, <code>inputs</code>, <code>outputs</code>, <code>when</code> clauses, and of course, <code>script</code>.</p> <pre><code>process &lt; name &gt; {\n  [ directives ]\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  when:\n    &lt; condition &gt;\n\n  script:\n  \"\"\"\n  &lt;script to be executed&gt;\n  \"\"\"\n}\n</code></pre> <p>A workflow is a composition of processes and dataflow logic.</p> <p>The workflow definition starts with the keyword <code>workflow</code>, followed by an optional name, and finally the workflow body delimited by curly braces.</p> <p>Let's review the structure of <code>hello-world.nf</code>, a toy example you will be executing and developing:</p> hello-world.nf<pre><code>process SAYHELLO {\n    debug true\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo 'Hello World!'\n    \"\"\"\n}\n\nworkflow {\n    SAYHELLO()\n}\n</code></pre> <p>The first piece of code (lines 1-11) describes a process called <code>SAYHELLO</code> with three definition blocks:</p> <ul> <li>debug: a directive that, when set to true, will print the output to the console</li> <li>output: directing outputs to be printed to <code>stdout</code> (standard output)</li> <li>script: the <code>echo 'Hello World!'</code> command</li> </ul> <p>The second block of code (13-15) lines describes the workflow itself, which consists of one call to the <code>SAYHELLO</code> process.</p> <p>Note</p> <p>Using <code>debug true</code> and <code>stdout</code> in combination will cause 'Hello World!' to be printed to the terminal.</p>"},{"location":"part1/03_hellonf/#commenting-your-code","title":"Commenting your code","text":"<p>It is worthwhile to comment your code so we, and others, can easily understand what the code is doing (you will thank yourself later).</p> <p>In Nextflow, a single line comment can be added by prepending it with two forward slash (<code>//</code>):</p> <pre><code>// This is my comment\n</code></pre> <p>Similarly, multi-line comments can be added using the following format:</p> <pre><code>/*\n * Use echo to print 'Hello World!' to standard out\n */\n</code></pre> <p>As a developer you can to choose how and where to comment your code.</p> <p>Exercise</p> <p>Add a comment to the pipeline to describe what the process block is doing</p> Solution <p>The solution may look something like this:</p> hello-world.nf<pre><code>/*\n * Use echo to print 'Hello World!' to standard out\n */\nprocess SAYHELLO {\n&lt;truncated&gt;\n</code></pre> <p>Or this:</p> hello-world.nf<pre><code>// Use echo to print 'Hello World!' to standard out\nprocess SAYHELLO {\n&lt;truncated&gt;\n</code></pre> <p>As a developer, you get to choose!</p>"},{"location":"part1/03_hellonf/#executing-hello-worldnf","title":"Executing <code>hello-world.nf</code>","text":"<p>The <code>nextflow run</code> command is used to execute Nextflow pipelines:</p> <pre><code>nextflow run &lt;pipeline.nf&gt;\n</code></pre> <p>When a pipeline is stored locally you need to supply the full path to the script. However, if the pipeline has been submitted to GitHub (and you have an internet connection) you can execute it without a local copy. For example, the hello repository hosted on the nextflow-io GitHub account can be executed using:</p> <pre><code>nextflow run nextflow-io/hello\n</code></pre> <p>Exercise</p> <p>Use the <code>nextflow run</code> command to execute <code>hello-world.nf</code></p> Solution <pre><code>nextflow run hello-world.nf\n</code></pre> <p>Yay! You have just run your first pipeline!</p> <p>Your console should look something like this:</p> <pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (1)\n[4e/6ba912] process &gt; SAYHELLO [100%] 1 of 1 \u2714\nHello World!\n</code></pre> <p>What does each line mean?</p> <ol> <li>The version of Nextflow that was executed</li> <li>The script and version names</li> <li>The executor used (in the above case: local)</li> <li>The first process is executed once, which means there is one task. The line starts with a unique hexadecimal value, and ends with the task completion information</li> <li>The result string from stdout is printed</li> </ol>"},{"location":"part1/03_hellonf/#task-directories","title":"Task directories","text":"<p>When a Nextflow pipeline is executed, a <code>work</code> directory is created. Processes are executed in isolated task directories. Each task uses a unique directory based on its hash (e.g., <code>4e/6ba912</code>) within the work directory.</p> <p>When a task is created, Nextflow stages the task input files, script, and other helper files into the task directory. The task writes any output files to this directory during its execution, and Nextflow uses these output files for downstream tasks and/or publishing.</p> <p>These directories do not share a writable state, and any required files or information must be passed through channels (this will be important later).</p> <p>Note</p> <p>You can execute <code>tree work</code> to view the work directory structure.</p> <p>Warning</p> <p>The work directory might not have the same hash as the one shown above.</p> <p>A series of files log files and any outputs are created by each task in the work directory:</p> <ul> <li><code>.command.begin</code>: Metadata related to the beginning of the execution of the process task</li> <li><code>.command.err</code>: Error messages (stderr) emitted by the process task</li> <li><code>.command.log</code>: Complete log output emitted by the process task</li> <li><code>.command.out</code>: Regular output (<code>stdout</code>) by the process task</li> <li><code>.command.sh</code>: The command that was run by the process task call</li> <li><code>.exitcode</code>: The exit code resulting from the command</li> </ul> <p>These files are created by Nextflow to manage the execution of your pipeline. While these file are not required now, you may need to interrogate them to troubleshoot issues later.</p> <p>Exercise</p> <p>Browse the <code>work</code> directory and view the <code>.command.sh</code> file</p> Solution <p>Note: The hash may be different to the example shown below.</p> <pre><code>cat work/4e/6ba9138vhsbcbsc83bcka/.command.sh\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to create a Nextflow pipeline</li> <li>How to interpret <code>hello-world.nf</code></li> <li>How to add comments to your pipelines</li> <li>How to <code>run</code> a Nextflow pipeline</li> <li>How to view log files create by Nextflow</li> </ol>"},{"location":"part1/04_output/","title":"Outputs","text":"<p>Learning objectives</p> <ol> <li>Utlizie Nextflow process output blocks</li> <li>Publish results from your pipeline with directives</li> </ol> <p>Instead of printing 'Hello World!' to the standard output it can be saved to a file. In a \"real-world\" pipeline, this is like having a command that specifies an output file as part of its normal syntax.</p> <p>Here you're going to update the <code>script</code> and the <code>output</code> definition blocks to save the 'Hello World!' as an output.</p>"},{"location":"part1/04_output/#redirecting-outputs","title":"Redirecting outputs","text":"<p>The script block will need to be updated to redirect the 'Hello World!' output to a file.</p> <p>The <code>&gt;</code> operator can be used for output redirection.</p> <p>Exercise</p> <p>Redirect 'Hello World!' to a file named 'output.txt' in the <code>script</code> block and add a comment to annotate your change.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    debug true\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"part1/04_output/#outputs-blocks","title":"Outputs blocks","text":"<p>Outputs in the output definition block typically require an output qualifier and a output name:</p> <pre><code>&lt;output qualifier&gt; &lt;output name&gt;\n</code></pre> <p>The output qualifier defines the type of data to be received. This information is used by Nextflow to apply the semantic rules associated with each qualifier, and handle it properly.</p> <p>Common output qualifiers include <code>val</code> and <code>path</code>:</p> <ul> <li><code>val</code>: Emit the variable with the specified name</li> <li>For example, <code>val 'Hello World!'</code></li> <li><code>path</code>: Emit a file produced by the process with the specified name</li> <li>For example, <code>path 'output.txt'</code></li> </ul> <p>See the Nextflow documentation for a full list of output qualifiers.</p> <p>Warning</p> <p>If you set the wrong qualifier the pipeline will likely throw errors.</p> <p>The output name is a name given to the output variable. If a specific file is being produced it can be named in single quotes:</p> hello-world.nf<pre><code>output:\npath 'output.txt'\n</code></pre> <p>The output name and the file generated by the script must match (or be picked up by a glob pattern).</p> <p>Exercise</p> <p>Add <code>path 'output.txt'</code> in the <code>SAYHELLO</code> output block.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    debug true\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>Warning</p> <p>This example is brittle because the output filename is hardcoded in two separate places (the <code>script</code> and the <code>output</code> definition blocks). If you change one but not the other, the script will break.</p>"},{"location":"part1/04_output/#publishing-directory","title":"Publishing directory","text":"<p>Without a publishing strategy any files that are created by a process will only exist in the <code>work</code> directory.</p> <p>Realistically, you may want to capture a set of outputs and save them in a specific directory.</p> <p>The <code>publishDir</code> directive can be used to specify where and how output files should be saved. For example:</p> <pre><code>publishDir 'results'\n</code></pre> <p>By adding the above to a process, all output files would be saved in a new folder called <code>results</code> in the current working directory. The process directive is process specific.</p> <p>Exercise</p> <p>Replace <code>debug true</code> with <code>publishDir 'results'</code> in the <code>SAYHELLO</code> process block. Execute the pipeline again. View your new <code>results</code> folder in the working directory.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to redirect outputs</li> <li>How to use <code>output</code> block</li> <li>How to publish results</li> </ol>"},{"location":"part1/05_inputs/","title":"Inputs","text":"<p>Learning objectives</p> <ol> <li>Describe Nextflow channel types</li> <li>Utlizie Nextflow process input blocks</li> </ol> <p>So far, you've been emitting a greeting ('Hello World!') that has been hardcoded into the script block. In a more realistic situation, you might want to pass a variable input to your script, much like you pass files to command line tools for analysis.</p> <p>Here you're going to to add some flexibility by introducing channels to your workflow and an input definition to your <code>SAYHELLO</code> process.</p>"},{"location":"part1/05_inputs/#channels","title":"Channels","text":"<p>In Nextflow, processes primarily communicate through channels.</p> <p>Channels are created using channel factories.</p> <p>There are numerous types of channel factories which can be utilized for creating different channel types and data types.</p> <p>Importantly, there are two kinds of channels (queue channels and value channels) which behave differently.</p> <p>Queue channel</p> <ul> <li>A non-blocking unidirectional first-in first-out queue connecting a producer process (i.e. outputting a value) to a consumer process, or an operators.</li> <li>Can be consumed only once.</li> </ul> <p>Value channel</p> <ul> <li>Can be bound (i.e. assigned) with one and only one value.</li> <li>Can be consumed any number of times.</li> </ul> <p>You're going to start by creating a channel that will contain your greeting with the <code>Channel.of()</code> channel factory.</p> <p>Note</p> <p>You can build different kinds of channels depending on the shape of the input data.</p>"},{"location":"part1/05_inputs/#channelof","title":"Channel.of()","text":"<p>The <code>Channel.of</code> method allows us to create a channel that emits the arguments provided to it. For example:</p> <pre><code>ch_greeting = channel.of('Hello World!')\n</code></pre> <p>Would create a channel (named <code>ch_greeting</code>) that contains the string 'Hello World!'</p> <p>Channels need to be created within the <code>workflow</code> definition.</p> <p>Exercise</p> <p>Create a channel named <code>greeting_ch</code> with the 'Hello World!' greeting.</p> Solution hello-world.nf<pre><code>workflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // Emit a greeting\n    SAYHELLO()\n}\n</code></pre>"},{"location":"part1/05_inputs/#input-definition-blocks","title":"Input definition blocks","text":"<p>Before <code>greeting_ch</code> can be passed to the <code>SAYHELLO</code> process as an input, you must first add an input block in the process definition.</p> <p>The inputs in the input block, much like the output block, must have a qualifier and a name:</p> <pre><code>&lt;input qualifier&gt; &lt;input name&gt;\n</code></pre> <p>Input names can be treated like a variable, and while the name is arbitrary, it should be recognizable.</p> <p>No quote marks are needed for variable inputs.</p> <pre><code>val greeting\n</code></pre> <p>Exercise</p> <p>Add an <code>input</code> block to the <code>SAYHELLO</code> process  with an input. Update the comment at the same time.</p> Solution hello-world.nf<pre><code>// Use echo to print a string and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo 'Hello World!' &gt; output.txt\n    \"\"\"\n}\n</code></pre> <p>The <code>SAYHELLO</code> process is now expecting an input value.</p> <p>The <code>greeting_ch</code> channel can now be supplied to <code>SAYHELLO()</code> process within the workflow block:</p> <pre><code>SAYHELLO(greeting_ch)\n</code></pre> <p>Without this, Nextflow will throw an error.</p> <p>Exercise</p> <p>Add the <code>greeting_ch</code> as an input for the <code>SAYHELLO</code> process.</p> Solution hello-world.nf<pre><code>workflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n}\n</code></pre> <p>The final piece is to update the <code>script</code> block to use the <code>input</code> value.</p> <p>For an input to be treated like a variable in the script block, a <code>$</code> must be prepended to the input name:</p> <pre><code>echo '$greeting' &gt; output.txt\n</code></pre> <p>The <code>'</code> around <code>$greeting</code> are required to treat the greeting as a single string.</p> <p>Exercise</p> <p>Update <code>hello-world.nf</code> to use the greeting input.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of('Hello world!')\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n}\n</code></pre> <p>Note</p> <p>The number of inputs in the input block and the workflow must match! If you had multiple inputs they would be listed across multiple lines in the process input block and listed inside the brackets in the workflow block.</p> Example example.nf<pre><code>process MYFUNCTION {\n    debug true\n\n    input:\n    val input_1\n    val input_2\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo $input_1 $input_2\n    \"\"\"\n}\n\nworkflow {\n    MYFUNCTION('Hello', 'World!')\n}\n</code></pre> <p>Yes! Your pipeline now uses an input channel!</p> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to use Channel factories</li> <li>How to how to add process inputs</li> </ol>"},{"location":"part1/06_params/","title":"Parameters","text":"<p>Learning objectives</p> <ol> <li>Implement pipeline parameters</li> <li>Understand the importance of parameters for flexible pipelines</li> </ol> <p>Parameters are constructs that can hold command line arguments.</p> <p>Here you're going to update the script with parameters to make it more flexible.</p>"},{"location":"part1/06_params/#why-are-parameters-useful","title":"Why are parameters useful?","text":"<p>Nextflow has multiple levels of configuration and, as different levels may have conflicting settings, they are ranked in order of priority and some configuration can be overridden.</p> <p>Parameters are useful because they can be set with a default value in a script but can then be overwritten at runtime using a flag. Simply, parameters allow us to configure some aspect of a pipeline without editing the script itself.</p> <p>Parameters can be created by prefixing a parameter name with the parameters scope (e.g., <code>params.greeting</code>) and are accessible by processes and workflows. They can be modified when you run your pipeline by adding a double hyphen (<code>--</code>) to the start of the parameter name (<code>--greeting</code>) and adding it to an execution command:</p> <pre><code>nextflow run hello-world.nf --greeting 'Hey'\n</code></pre>"},{"location":"part1/06_params/#-greeting","title":"<code>--greeting</code>","text":"<p>Instead of hard coding 'Hello World!' as an input, a parameter, with a default value, can be created:</p> <pre><code>params.greeting = 'Hello World!'\n</code></pre> <p>The parameter can then be used in a channel factory (just like the hard coded string):</p> <pre><code>greeting_ch = Channel.of(params.greeting)\n</code></pre> <p>The parameter can then be flexibly changed using a <code>--greeting</code> flag in the run command:</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>Exercise</p> <p>Update the <code>hello-world.nf</code> script to use a <code>greeting</code> parameter as an input. Define the default for the <code>greeting</code> parameter at the top of the script and give it the default value <code>'Hello World!'</code>.</p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n}\n</code></pre> <p>The <code>hello-world.nf</code> pipeline can now be executed with the <code>--greeting</code> flag and a custom greeting:</p> <pre><code>nextflow run hello-world.nf --greeting 'Bonjour le monde!'\n</code></pre>"},{"location":"part1/06_params/#-outdir","title":"<code>--outdir</code>","text":"<p>It isn't very convenient to have the same output directory created every time you run your pipeline as the results are being overwritten.</p> <p>Instead, a parameter can be used so you can change the publishing directory for every execution:</p> <pre><code>publishDir params.outdir\n</code></pre> <p>A default value can be used for convenience as Nextflow will throw and error if <code>publishDir</code> is set to <code>null</code>.</p> <pre><code>params.outdir = 'results'\n</code></pre> <p>However, you may consider having no default value here and letting the pipeline fail to prevent the accidental overwriting of results.</p> <p>Exercise</p> <p>Update the <code>hello-world.nf</code> script to use an <code>outdir</code> parameter as the publishing directory. Define the default for the <code>outdir</code> parameter at the top of the script and give it the default value <code>'results'</code>.</p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Set default output directory\nparams.outdir = 'results'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir params.outdir\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n}\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to how to add a parameter to a pipeline</li> <li>How to modify a parameter using the command line</li> </ol>"},{"location":"part1/07_process/","title":"Adding processes","text":"<p>Learning objectives</p> <ol> <li>Transform text strings </li> <li>Chain Nextflow processes together</li> <li>Utilize process outputs as inputs</li> </ol> <p>Up until now you've been modifying a single step. However, pipelines generally consist of multiple steps where outputs from one step are used as inputs for the next.</p> <p>Here you're going to step things up again and add another process to the pipeline.</p>"},{"location":"part1/07_process/#translating-text","title":"Translating text","text":"<p>The <code>tr</code> command is a UNIX command-line utility for translating or deleting characters. It supports a range of transformations including uppercase to lowercase, squeezing repeating characters, deleting specific characters, and basic find and replace. It can be used with UNIX pipes to support more complex translation. <code>tr</code> stands for translate.</p> <pre><code>tr '[a-z]' '[A-Z]'`\n</code></pre>"},{"location":"part1/07_process/#piping-commands","title":"Piping commands","text":"<p>The pipe command in Linux, represented by the vertical bar symbol <code>|</code>, is an essential tool for command-line enthusiasts and professionals alike. The primary purpose of the pipe command is to connect the output of one command directly into the input of another:</p> <pre><code>echo 'Hello World' | tr '[a-z]' '[A-Z]'\n</code></pre> <p>The contents of a file can be piped using the <code>cat</code> command:</p> <pre><code>cat output.txt | tr '[a-z]' '[A-Z]'\n</code></pre> <p>Like before, the output can be redirected to an output file:</p> <pre><code>cat output.txt | tr '[a-z]' '[A-Z]' &gt; upper.txt\n</code></pre>"},{"location":"part1/07_process/#converttoupper","title":"<code>CONVERTTOUPPER</code>","text":"<p>The output of the <code>SAYHELLO</code> process is a text file called <code>output.txt</code>.</p> <p>In the next step of the pipeline, you will add a new process named convert <code>CONVERTTOUPPER</code> that will convert all of the lower case letters in this file to a uppercase letters and save them as a new file.</p> <p>The <code>CONVERTTOUPPER</code> process will follow the same structure as the <code>SAYHELLO</code> process:</p> <pre><code>process CONVERTTOUPPER {\n    publishDir params.outdir\n\n    input:\n    &lt;input qualifier&gt; &lt;input name&gt;\n\n    output:\n    &lt;output qualifier&gt; &lt;output name&gt;\n\n    script:\n    \"\"\"\n    &lt;script&gt;\n    \"\"\"\n}\n</code></pre> <p>Using what you have learned in the previous sections you will now write a new process using the <code>tr</code> command from above.</p> <p>Exercise</p> <p>Add new process named <code>CONVERTTOUPPER</code> that will take an input text file, convert all of the lowercase letters in the text file to uppercase letters, and save a new text file that contains the translated letters.</p> Hint: <code>input:</code> <pre><code>path input_file\n</code></pre> <p>Hint 1: The input is a file and requires the <code>path</code> qualifier.</p> <p>Hint 2: The input name is <code>input_file</code>, however, you may call it something different.</p> Hint: <code>output:</code> <p>The output</p> <pre><code>path 'upper_output.txt'\n</code></pre> <p>Hint 1: The output is a file and requires the <code>path</code> qualifier.</p> <p>Hint 2: The output name is hard coded as 'upper.txt', however, you may call it something different.</p> Hint: <code>script:</code> <p>The script might look something like this:</p> <pre><code>cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper.txt\n</code></pre> <p>Hint 1: <code>input_file</code> must be the same as what was specified as the input name in the input block.</p> <p>Hint 2: The output text file is named <code>upper.txt</code></p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Set a default output directory\nparams.outdir = 'results'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir params.outdir\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    publishDir params.outdir\n\n    input:\n        path input_file\n\n    output:\n        path 'upper.txt'\n\n    script:\n    \"\"\"\n    cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n}\n</code></pre>"},{"location":"part1/07_process/#connecting-processes","title":"Connecting processes","text":"<p>Outputs from one process can be used as inputs for another.</p> <p>Outputs from a process can be accessed by adding <code>.out</code> to the end of a process name in the workflow definition:</p> <pre><code>SAYHELLO.out\n</code></pre> <p>Outputs can then be used as an input for another process:</p> <pre><code>CONVERTTOUPPER(SAYHELLO.out)\n</code></pre> <p>The same output could be used as inputs for multiple processes.</p> <p>Warning</p> <p>Adding <code>.out</code> to the end of a process name only works for single outputs. If there are multiple outputs the <code>emit</code> option must be used. See additional options for more information.</p> <p>Exercise</p> <p>Add the <code>CONVERTTOUPPER</code> process to your workflow definition. Use the output from <code>SAYHELLO</code> as its input.</p> Solution hello-world.nf<pre><code>// Set default greeting\nparams.greeting = 'Hello World!'\n\n// Set a default output directory\nparams.outdir = 'results'\n\n// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir params.outdir\n\n    input:\n    val greeting\n\n    output:\n    path 'output.txt'\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; output.txt\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    publishDir params.outdir\n\n    input:\n        path input_file\n\n    output:\n        path 'upper.txt'\n\n    script:\n    \"\"\"\n    cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper.txt\n    \"\"\"\n}\n\nworkflow {\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n    // Convert the greeting to uppercase\n    CONVERTTOUPPER(SAYHELLO.out)\n\n}\n</code></pre> <p>Executing <code>hello-world.nf</code> will now show a second step:</p> <pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (2)\n[ef/b99a2f] SAYHELLO (1)       [100%] 1 of 1 \u2714\n[cd/c8cf1b] CONVERTTOUPPER (1) [100%] 1 of 1 \u2714\n</code></pre> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to translate strings</li> <li>How add more processes to a script</li> <li>How to use outputs and inputs</li> </ol>"},{"location":"part1/08_dynamic/","title":"Dynamic naming","text":"<p>Learning objectives</p> <ol> <li>Dynamically name variables in Nextflow</li> </ol> <p>Currently, the outputs of the <code>SAYHELLO</code> and <code>CONVERTTOUPPER</code> processes are being saved as <code>output.txt</code> and <code>upper.txt</code>, respectively.</p> <p>In some situations this would be fine. However, to help identify the outputs you want your outputs to be dynamic.</p> <p>Let's get tricky and name your output files dynamically.</p>"},{"location":"part1/08_dynamic/#dynamic-outputs","title":"Dynamic outputs","text":"<p>When an output file name needs to be expressed dynamically, it is possible to define it using a dynamic string that references values defined in the input declaration block or in the script global context.</p> <p>For example, the <code>SAYHELLO</code> input value <code>greeting</code> can be used to help name the output file.</p> <pre><code>process SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path \"${greeting}.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; ${greeting}.txt\n    \"\"\"\n}\n</code></pre> <p>Curly brackets <code>{}</code> have been used to wrap <code>greeting</code> in the <code>output</code> and <code>script</code> block so it is interpreted as a variable as a part of a file name.</p> <p>There is an important difference between single-quoted (<code>'</code>) and double-quoted (<code>\"</code>)strings. Double-quoted strings support variable interpolations while single-quoted strings do not.</p> <p>Exercise</p> <p>Update the <code>SAYHELLO</code> and <code>CONVERTTOUPPER</code> process to use dynamic output names.</p> <p>Warning</p> <p>It's difficult to name a file with a space. Use a simple greeting without spaces, e.g., \"Hello\", when testing your pipeline.</p> Solution hello-world.nf<pre><code>// Use echo to print 'Hello World!' and redirect to output.txt\nprocess SAYHELLO {\n    publishDir 'results'\n\n    input:\n    val greeting\n\n    output:\n    path \"${greeting}.txt\"\n\n    script:\n    \"\"\"\n    echo '$greeting' &gt; ${greeting}.txt\n    \"\"\"\n}\n\n// Use tr to convert lowercase letters to upper case letters and save as upper.txt\nprocess CONVERTTOUPPER {\n    publishDir 'results'\n\n    input:\n        path input_file\n\n    output:\n        path \"upper_${input_file}\"\n\n    script:\n    \"\"\"\n    cat $input_file | tr '[a-z]' '[A-Z]' &gt; upper_${input_file}\n    \"\"\"\n}\n\nworkflow {\n\n    // Set default greeting\n    params.greeting = \"Hello\"\n\n    // Create a channel for inputs\n    greeting_ch = Channel.of(params.greeting)\n\n    // Emit a greeting\n    SAYHELLO(greeting_ch)\n\n    // Convert the greeting to uppercase\n    CONVERTTOUPPER(SAYHELLO.out)\n\n}\n</code></pre> <p>Let's execute your pipeline and view the changes to see if your outputs have been named dynamically.</p> <pre><code>nextflow run hello-world.nf --greeting 'Hello'\n</code></pre> <p>While the output will look the same:</p> <pre><code>N E X T F L O W  ~  version 23.10.1\nLaunching `hello-world.nf` [mighty_murdock] DSL2 - revision: 80e92a677c\nexecutor &gt;  local (2)\n[ef/b99a2f] SAYHELLO (1)       [100%] 1 of 1 \u2714\n[cd/c8cf1b] CONVERTTOUPPER (1) [100%] 1 of 1 \u2714\n</code></pre> <p>You should now see some new files in your results folder:</p> <ul> <li><code>Hello.txt</code></li> <li><code>upper_Hello.txt</code></li> </ul> <p>Summary</p> <p>In this step you have learned:</p> <ol> <li>How to utilize dynamic naming</li> <li>How to use curly brackets (<code>{}</code>)</li> <li>How to use single (<code>'</code>) and double (<code>\"</code>) quotes</li> </ol>"},{"location":"part2/00_intro/","title":"2.0 Introduction","text":"<p>Part 2 builds on fundamental concepts learned in Part 1 and provides you with  hands-on experience in Nextflow workflow development. Throughout the session  we will be working with a bulk RNAseq dataset to build our workflow. </p> <p>We will construct channels that control how our data flows through  processes that we will progressively construct to build our workflow.  Each lesson in Part 2 will build on the previous lessons, so you can gain a  deeper understanding of the techniques and the impact they have on your  resulting workflow. </p> <p>In Part 2 of this workshop, we will explore a scenario of creating a multi-sample  Nextflow workflow for preparing RNAseq data. We will build the workflow,  step-by-step, by converting a series of provided bash scripts into small workflow  components.  </p> <p>Along the way, you will encounter Nextflow concepts (from Part 1, and some new) and our best practice recommendations for developing your own pipeline.  </p> <p>Part 2 is based off the  Simple RNA-Seq workflow training material developed by Seqera.  </p>"},{"location":"part2/00_intro/#201-log-back-into-your-instance","title":"2.0.1 Log back into your instance","text":"<p>Re-connect to your Virtual Machine by following the  \"Connect to the VM\" section from the setup page.</p> <p>Once connected, in your VSCode terminal, change directories into the <code>part2/</code> directory:  </p> <pre><code>cd ~/part2/\n</code></pre> <p>All Part 2 activities will be conducted in this folder. </p>"},{"location":"part2/00_intro/#202-our-scenario-from-bash-scripts-to-scalable-workflows","title":"2.0.2 Our scenario: from bash scripts to scalable workflows","text":"<p>Imagine you are a bioinformatician in a busy research lab. Your team will be receiving a large batch of samples that need to be processed through a series of analysis steps.  </p> <p>You have inherited a set of bash scripts from a former colleague, which were used to process a handful of samples manually. These scripts are robust and well-tested, but they were not designed with scalability in mind.  </p> <p>As more samples come in, running these scripts one by one is becoming increasingly tedious and error-prone.  </p> <p>You need a way to automate this process, ensuring consistency and efficiency across many samples. </p> <p>You decide to use Nextflow.  </p> <p>Exercise</p> <p>View the bash scripts your colleague provided:</p> <ol> <li>Use either the VSCode File Explorer, or the integrated terminal to navgiate  to the <code>~/part2/bash_scripts/</code> directory.</li> <li>Inspect the scripts (open in a VSCode tab, or text editor in the terminal).</li> </ol> <p>Each script runs a single data processing step and are run in order of the prefixed number.</p> <p>Poll</p> <p>What are some limitations of these scripts in terms of running them in a pipeline and monitoring it?  </p>"},{"location":"part2/00_intro/#203-our-workflow-rnaseq-data-processing","title":"2.0.3 Our workflow: RNAseq data processing","text":"<p>Don't worry if you don't have prior knowledge of RNAseq!</p> <p>The focus of this workshop is on learning Nextflow, the RNAseq data we  are using in this part are just a practical example to help you understand  how the workflow system works. </p> <p>RNAseq is used to study gene expression and has many applications across biomedicine, agriculture and evolutionary studies. In our scenario we are going to  run through some basic core steps that allow us to explore different aspects of  Nextflow. </p>"},{"location":"part2/00_intro/#data","title":"Data","text":"<p>The data we will use includes:</p> <ul> <li><code>*.fastq</code>: Paired-end RNAseq reads from three different samples (gut, liver, lung).  </li> <li><code>transcriptome.fa</code>: A transcriptome file.  </li> <li><code>samplesheet*.csv</code>: CSV files that help us track which files belong to which samples.</li> </ul>"},{"location":"part2/00_intro/#tools","title":"Tools","text":"<p>We will be implementing and integrating three commonly used bioinformatics tools:  </p> <ol> <li>Salmon is a tool for quantifying molecules known as transcripts through RNA-seq data.  </li> <li>FastQC is a tool for quality analysis of high throughput sequence data. You can think of it as a way to assess the quality of your data.  </li> <li>MultiQC searches a given directory for analysis logs and compiles an HTML report for easy viewing. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.  </li> </ol> <p>These tools will be run using Docker containers. We will not explore how the data and tools work further, and focus on how they should be implemented in a Nextflow workflow.  </p>"},{"location":"part2/00_intro/#204-pipeline-structure-and-design","title":"2.0.4 Pipeline structure and design","text":"<p>Having reviewed the bash scripts, we've decided to keep its modular structure and will build the following four processes (discrete steps):</p> <ol> <li><code>INDEX</code> - Transcriptome indexing (tool: Salmon): create an index of the reference transcriptome for faster and efficient data processing.</li> <li><code>FASTQC</code> - Raw data quality control (tool: FastQC): assess the quality of fastq files to ensure our data is usable. </li> <li><code>QUANTIFICATION</code> - Gene quantification (tool: Salmon): counting how many reads map to each gene in the transcriptome. </li> <li><code>MULTIQC</code> - Summarise results in a report (tool: MultiQC): generate a report that summarises quality control and gene quantification results. </li> </ol> <p></p>"},{"location":"part2/00_intro/#205-nextflowing-the-workflow","title":"2.0.5 Nextflowing the workflow","text":"<p>Each lesson in part 2 of our workshop focuses on implementing one process of  the workflow at a time. We will iteratively build the workflow and processes  in a single <code>main.nf</code> file and lightly use a <code>nextflow.config</code> file for configuration.</p> <p></p>"},{"location":"part2/00_intro/#mainnf","title":"<code>main.nf</code>","text":"<p>The <code>main.nf</code> file is the core script that defines the steps of your Nextflow workflow. It outlines each <code>process</code> (the individual commands, or, data processing steps) and how they are connected to each other. This <code>main.nf</code> script focuses on what the workflow does.</p> <p>Most of the code you will write in this Part will go in <code>main.nf</code>. </p> <p>We will follow an ordered approach for each step of the workflow  building off the <code>process</code> structure from Part 1.3. You will be using this process template for each step of the workflow, adding  them to the <code>main.nf</code> script: </p> <pre><code>process &lt; name &gt; {\n  [ directives ]\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  &lt; script to be executed &gt;\n  \"\"\"\n}\n</code></pre>"},{"location":"part2/00_intro/#nextflowconfig","title":"<code>nextflow.config</code>","text":"<p>The <code>nextflow.config</code> file is a key part of any Nextflow workflow. While <code>main.nf</code> outlines the steps and processes of the workflow,  <code>nextflow.config</code> allows you to define important settings and configurations  that control how your workflow should run.</p> <p>This script will be intermittently used in the following lessons to control the use of Docker containers, how much resources (CPUs) should be used, and reporting of the workflow after it has finished running.</p>"},{"location":"part2/01_salmon_idx/","title":"2.1 Implementing a simple process with a container","text":"<p>Learning objectives</p> <ol> <li>Implement a Nextflow process that takes a single file as input.  </li> <li>Understand the importance of containers in ensuring consistent and reproducible execution across processes.</li> <li>Store output files with the <code>publishDir</code> directive.  </li> </ol> <p>In this lesson we will be implement <code>00_index.sh</code> as our first Nextflow process, <code>INDEX</code>. Here, we are working with the first step of the RNA-seq data processing workflow: indexing the transcriptome for downstream processes. To do this, we will need to run Salmon's indexing mode.    Open the bash script <code>00_index.sh</code>:  </p> 00_index.sh<pre><code>mkdir \"results\"\nsalmon index \\\n    --transcripts data/ggal/transcriptome.fa \\\n    --index results/salmon_index\n</code></pre> <ul> <li>The script first creates a <code>results/</code> folder then runs the <code>salmon index</code> command.  </li> <li>The Salmon <code>--transcripts</code> flag indicates that the path to the input transcriptome file is <code>data/ggal/transcriptome.fa</code>.  </li> <li>The Salmon <code>--index results/salmon_index</code> flag tells <code>salmon</code> to save the output index files in a directory called <code>salmon_index</code>, within the newly created <code>results</code> directory.  </li> </ul> <p>Avoid hardcoding arguments by using parameters</p> <p>The paths to the transcriptome file (<code>data/ggal/transcriptome.fa</code>) and the output directory (<code>results/salmon_index</code>) are hardcoded in this bash script. If you wanted to change the input transcriptome file or the output location, you must manually edit the script. This makes our scripts less flexible and easy to use. </p> <p>Nextflow addresses the issue of hardcoded paths by allowing parameters to be passed dynamically at runtime as parameters (<code>params</code>). </p>"},{"location":"part2/01_salmon_idx/#211-building-the-process","title":"2.1.1 Building the process","text":"<p>In the empty <code>main.nf</code> script, add the following <code>process</code> scaffold with the script definition:  </p> main.nf<pre><code>process INDEX {\n  [ directives ]\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  salmon index --transcripts $transcriptome --index salmon_index\n  \"\"\"\n}\n</code></pre> <p>It contains: </p> <ul> <li>The empty <code>input:</code> block for us to define the input data for the process. </li> <li>The empty <code>output:</code> block for us to define the output data for the process.</li> <li>The <code>script:</code> block prefilled with the command that will be executed.</li> </ul> <p>Info</p> <p>The process <code>script</code> block is executed as a Bash script by default. In Part 2 of the workshop, we will only be using Nextflow variables within the <code>script</code> block.</p> <p>Next, we will edit the <code>input</code> and <code>output</code> definitions to match the specific data and results for this process. In the <code>00_index.sh</code> script, the relevant information is:  </p> <ul> <li>The fasta (<code>.fa</code>) file defined by the variable <code>$transcriptome</code> and provided to the <code>--transcripts</code> flag  </li> <li>The index output directory <code>salmon_index/</code> provided to the <code>--index</code> flag  </li> </ul> <p>Defining inputs and outputs</p> <p>Remember, input and output definitions require a qualifier and name. For example: <pre><code>input:\n&lt;input qualifier&gt; &lt;input name&gt;\n\noutput:\n&lt;output qualifier&gt; &lt;output name&gt;\n</code></pre></p> <p>The qualifier defines the type of data, and the names are treated like variables.</p> main.nf<pre><code>process INDEX {\n  [ directives ]\n\n  input:\n  path transcriptome\n\n  output:\n  path 'salmon_index'\n\n  script:\n  \"\"\"\n  salmon index --transcripts $transcriptome --index salmon_index\n  \"\"\"\n}\n</code></pre> <p>Note that the input <code>path transcriptome</code> refers to a variable, meaning the actual file or directory provided as input can be changed depending on the data you provide it. The output <code>path 'salmon_index'</code> is fixed, meaning it will always create an output folder called <code>salmon_index</code>, no matter what the input is.  </p> <p>This is how Nextflow can handle different inputs while always producing the same output name.  </p> <p>More information on using input and output blocks can be found in the process inputs and outputs Nextflow documentation.  </p>"},{"location":"part2/01_salmon_idx/#212-save-files-to-an-output-directory-with-publishdir","title":"2.1.2 Save files to an output directory with <code>publishDir</code>","text":"<p>Next we will implement the Nextflow equivalent of saving the output files into a <code>results/</code> directory.  </p> <p>Replace <code>[ directives ]</code> in your <code>main.nf</code> script with the <code>publishDir</code>  directive, specifying the directory name as <code>\"results\"</code> and the mode as <code>'copy'</code>. Your <code>main.nf</code> should look like this: </p> main.nf<pre><code>process INDEX {\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path transcriptome\n\n  output:\n  path 'salmon_index'\n\n  script:\n  \"\"\"\n  salmon index --transcripts $transcriptome --index salmon_index\n  \"\"\"\n}\n</code></pre> <p>This process is now directed to copy all output files into a <code>results/</code> directory. This saves having to specify the output directory in the script definition each process, or a tedious <code>mv salmon_index/ results/</code> step. </p> <p>Nextflow also handles whether the directory already exists or if it should be created. In the <code>00_index.sh</code> script you had to manually make a  results directory with <code>mkdir -p \"results</code>.</p> <p>More information and other modes can be found on publishDir.</p>"},{"location":"part2/01_salmon_idx/#213-using-containers","title":"2.1.3 Using containers","text":"<p>Nextflow recommends using containers to ensure reproducibility and portability of your workflow. Containers package all the software and dependencies needed for each tool into a self-contained environment. This means you don\u2019t have to manually install anything on your system, and your workflow will work consistently across different systems \u2014 whether you're running it on your local machine, a cluster, or in the cloud. Containers make it easier to share your workflow with others and ensure it runs the same way every time, no matter where it's executed.  </p> <p>Nextflow supports multiple container runtimes. In this workshop, we'll be demonstrating the value containers can bring to your workflow by using Docker.</p> Tip: different tools for different purposes <p>In this workshop, we're using Docker to run containers. However, for some systems like HPC where you won't have administrative access to your environment, other options like Singularity/Apptainer will be more suitable.</p> <p>You don't have to write your own containers to run in your workflow. There are many container repositories out there. We highly recommend using  Biocontainers wherever possible. Biocontainers are pre-built and tested containers specifically for bioinformatics tools. They have a huge library and great community support. </p> <p>You can find Biocontainers at the following repositories:  </p> <ul> <li>Biocontiners registry</li> <li>Quay.io</li> <li>DockerHub</li> <li>Seqera containers</li> </ul> <p>Add the following container directive to the <code>INDEX</code> process, above <code>publishDir</code>:  </p> <p>In Nextflow, we can specify that a process should be run within a specified container using the container directive.  </p> <p>Add the following container directive to the <code>INDEX</code> process, above <code>publishDir</code>: main.nf<pre><code>process INDEX {\n    container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n    publishDir \"results\" mode: 'copy'\n\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --transcripts $transcriptome --index salmon_index\n    \"\"\"\n}\n</code></pre></p> <p>You now have a complete process! </p> <p>Usually, containers need to be downloaded using a command such as <code>docker pull [image]</code>. All containers have been previously downloaded for the workshop beforehand.  </p> Tip: use one container per process <p>Using single containers for each process in your workflow is considered best practices for the following reasons:</p> <ul> <li>Flexibility: different processes require different tools (or versions). By using separate containers, you can easily tailor the container to the needs of each process without conflicts.</li> <li>Build and run efficiency: Smaller, process-specific containers are faster to load and run compared to one large container that has unnecessary tools or dependencies for every process.</li> <li>Easier Maintenance: it\u2019s easier to update or modify one container for a specific process than to manage a large, complex container with many tools.</li> <li>Reproducibility: reduces the risk of issues caused by software conflicts.</li> </ul> <p>Before we can run the workflow, we need to tell Nextflow to run containers using Docker. Nextflow requires Docker to be installed on your system in order for this to work. Docker has been  pre-installed on your Virtual Machine.  </p> <p>We can tell Nextflow configuration to run containers with Docker by using the  <code>nextflow.config</code> file.</p> <p>Create a <code>nextflow.config</code> file in the same directory as <code>main.nf</code>.  </p> <p>Note</p> <p>You can create the file via the VSCode Explorer (left sidebar) or in the terminal with a text editor.</p> <p>If you are using the Explorer, right click on <code>part2</code> in the sidebar and select \"New file\".</p> <p>Add the following line to your config file:</p> nextflow.config<pre><code>docker.enabled = true\n</code></pre> <p>You have now configured Nextflow to use Docker.  </p> <p>Tip</p> <p>Remember to save your files after editing them!</p>"},{"location":"part2/01_salmon_idx/#214-adding-params-and-the-workflow-scope","title":"2.1.4 Adding <code>params</code> and the workflow scope","text":"<p>Now that you have written your first Nextflow process, we need to prepare it for execution.  </p> <p>You can think of Nextflow processes as similar to a function definition in R or Python. We have defined what the process should do, but to actually run it, we need to call the process within the workflow and pass in the inputs.</p> <p>To run the process, we need to call it inside the <code>workflow{}</code> block, where we control how data flows through the pipeline. To provide the input data we need to define parameters. </p> <p>In the <code>00_index.sh</code> script, the file <code>data/ggal/transcriptome.fa</code> was passed as the input into <code>salmon index</code>.  </p> <p>We will pass in this file path with the <code>params</code> scope. Add the following to  the top of your <code>main.nf</code> script:  </p> main.nf<pre><code>// pipeline input parameters\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\n</code></pre> <p>Implicit variables in Nextflow</p> <p>Nextflow provides a set of implicit variables that can be used in your workflows. These variables are predefined and can be used to access information about the workflow environment, configuration, and tasks. </p> <p>We will use <code>$projectDir</code> to indicates the directory of the <code>main.nf</code> script. This is defined by Nextflow as the directory where the <code>main.nf</code> script is located.</p> <p>The <code>params</code> and <code>process</code> names do not need to match!</p> <p>In the <code>INDEX</code> process, we defined the input as a path called <code>transcriptome</code>, whereas the parameter is called <code>transcriptome_file</code>. These do not need to be identical names as they are called in different scopes (the <code>INDEX</code> process scope, and <code>workflow</code> scope, respectively).</p> <p>Recall that parameters are inputs and options that can be customised when the workflow is  executed. They allow you to control things like file paths and options for  tools without changing the process code itself.  </p> <p>We defined a default value for <code>params.transcriptome</code> in the <code>main.nf</code> script.  If we need to run our pipeline with a different transcriptome  file, we can overwrite this default in our execution command with  <code>--transcriptome</code> double hyphen flag.</p> <p>Next, add the workflow scope at the bottom of your <code>main.nf</code> after the process:  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n}\n</code></pre> <p>This will tell Nextflow to run the <code>INDEX</code> process with <code>params.transcriptome_file</code> as input.</p> <p>Tip: Your own comments</p> <p>As a developer you can to choose how and where to comment your code! Feel free to modify or add to the provided comments to record useful information about the code you are writing.</p> <p>We are now ready to run our workflow!  </p>"},{"location":"part2/01_salmon_idx/#215-running-the-workflow","title":"2.1.5 Running the workflow","text":"<p>In the terminal, run the command:  </p> <pre><code>nextflow run main.nf\n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>N E X T F L O W   ~  version 24.04.4\n\nLaunching `main.nf` [chaotic_jones] DSL2 - revision: 6597720332\n\nexecutor &gt;  local (1)\n[de/fef8c4] INDEX | 1 of 1 \u2714\n</code></pre> <p>Recall that the specifics of the output are randomly generated (i.e. <code>[chaotic_jones]</code> and <code>[de/fef8c4]</code> in this example).</p> <p>In this example, the output files for the <code>INDEX</code> process is output in <code>work/de/fef8c4...</code>.</p> <p>You have successfully run your first workflow!  </p>"},{"location":"part2/01_salmon_idx/#216-inspecting-the-outputs","title":"2.1.6 Inspecting the outputs","text":"<p>To observe exactly what command is being run for a process, we can attempt  to infer this information from the process definition in our <code>main.nf</code>  script. Given all the different parameters that may be applied at the process  level, it may not be very clear exactly what inputs are being fed in.</p> <p>For more complex commands, it can be very hard to see what is actually  happening in the code, given all the different variables and conditional  arguments inside a process. </p> <p>Hidden files in the work directory</p> <p>Remember that the pipeline\u2019s results are cached in the work directory. In addition to the cached files, each task execution directories inside the work directory contains a number of hidden files:</p> <ul> <li><code>.command.sh</code>: The command script run for the task.</li> <li><code>.command.run</code>: The command wrapped used to run the task.</li> <li><code>.command.out</code>: The task\u2019s standard output log.</li> <li><code>.command.err</code>: The task\u2019s standard error log.</li> <li><code>.command.log</code>: The wrapper execution output.</li> <li><code>.command.begin</code>: A file created as soon as the job is launched.</li> <li><code>.exitcode</code>: A file containing the task exit code (0 if successful)</li> </ul> <p>Instead of trying to infer how the variable is being defined and applied to  the process, let\u2019s use the hidden command files saved for this task in the work/ directory.</p> <p>Exercise</p> <ol> <li>Navigate to the <code>work/</code> directory and open the <code>.command.sh</code> file.</li> <li>Compare the <code>.command.sh</code> file with <code>00_index.sh</code>.  </li> </ol> <p>Poll</p> <p>In <code>.command.sh</code>, there are no longer hardcoded file paths (e.g. <code>results/salmon_index</code> and <code>data/ggal.transcriptome.fa</code>). What Nextflow directive and scope enable this?</p> <p>Summary</p> <p>In this lesson you have learned:  </p> <ol> <li>How to implement a simple process with one input file  </li> <li>How to define parameters in your workflow scripts and the command line</li> <li>How to use configure a process to run using a container   </li> <li>How to output files in a dedicated <code>publishDir</code> </li> </ol>"},{"location":"part2/02_fastqc/","title":"2.2 Samplesheets, operators, and groovy","text":"<p>Learning objectives</p> <ol> <li>Implement a process with a tuple input.</li> <li>Understand why samplesheets should be used to read in data.</li> <li>Build an input channel using operators and Groovy.</li> </ol> <p>In this lesson we will transform the next bash script, <code>01_fastqc.sh</code> into a process called <code>FASTQC</code>. This step focuses on the next phase of RNAseq data processing: assessing the quality of some our raw sequencing reads. </p> <p>To do this, we will need to run FastQC  over pairs of fastq files. </p> <p></p> <p>Our goal in porting these bash scripts to Nextflow is to build a workflow that can scale to run on multiple samples with minimal intervention. To do this, we will use a samplesheet, allowing us to provide multiple samples and their corresponding fastq files to our Nextflow workflow. </p> <p>Building channels in Nextflow can be tricky. Depending on what data you need  to capture and how you want to organise it you will likely need to use  operators to manipulate your channel. Sometimes operators alone won't be enough, and you'll need to also use Groovy (Nextflow's underlying programming language) to capture pertinent information.  </p> <p>Since this is an advanced task, we will provide you with all the code you need. Although Nextflow does not yet offer a built-in operator for reading samplesheets, their use is widespread in bioinformatics workflows. </p> <p>Open the bash script <code>01_fastqc.sh</code>:  </p> 01_fastqc.sh<pre><code>SAMPLE_ID=gut\nREADS_1=\"data/ggal/${SAMPLE_ID}_1.fq\"\nREADS_2=\"data/ggal/${SAMPLE_ID}_2.fq\"\n\nmkdir -p \"results/fastqc_${SAMPLE_ID}_logs\"\nfastqc \\\n    --outdir \"results/fastqc_${SAMPLE_ID}_logs\" \\\n    --format fastq ${READS_1} ${READS_2}\n</code></pre> <p>There's a lot going on in this script, let's break it down.</p> <p><code>SAMPLE_ID=gut</code> assigns \"gut\" to the bash variable<code>SAMPLE_ID</code>. This is used to:  </p> <ul> <li>Avoid hardcoding the sample name multiple times in the script  </li> <li>Ensure that file pairs of the same sample are processed together  </li> <li>Ensure that this script can be run on different sample pairs  </li> </ul> <p><code>READS_1</code> and <code>READS_2</code> specify the paths to the gut <code>.fq</code> files.  </p> <p>Similar to the bash script in the previous step (<code>00_index.sh</code>), <code>mkdir -p</code> creates an output folder so that the <code>fastqc</code> outputs can be saved here.  </p> <p>In the <code>fastqc</code> command,</p> <ul> <li><code>--outdir</code> specifies the name of the output directory</li> <li><code>--format</code> is a required flag to indicate what format the the reads are in</li> <li><code>${READS_1}</code> and <code>${READS_2}</code> propagate the paths of the <code>.fq</code> files  </li> </ul>"},{"location":"part2/02_fastqc/#221-building-the-process","title":"2.2.1 Building the process","text":""},{"location":"part2/02_fastqc/#1-process-directives","title":"1. Process directives","text":"<p>Start by adding the following <code>process</code> scaffold and script definition to your <code>main.nf</code> under the INDEX process code but before the <code>workflow{}</code> block:  </p> main.nf<pre><code>process FASTQC {\n  container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  mkdir -p \"fastqc_${sample_id}_logs\"\n  fastqc --outdir \"fastqc_${sample_id}_logs\" --format fastq $reads_1 $reads_2\n  \"\"\"\n}\n</code></pre> <p>It contains: </p> <ul> <li>Prefilled process directives <code>container</code> and <code>publishDir</code>.</li> <li>The empty <code>input:</code> block for us to define the input data for the process. </li> <li>The empty <code>output:</code> block for us to define the output data for the process.</li> <li>The <code>script:</code> block prefilled with the command that will be executed.</li> </ul> <p>Dynamic naming</p> <p>Recall that curly brackets are used to pass variables as part of a file name.</p>"},{"location":"part2/02_fastqc/#2-define-the-process-output","title":"2. Define the process <code>output</code>","text":"<p>Unlike <code>salmon</code> from the previous process, <code>fastqc</code> requires that the output directory be created before running the command, hence the requirement to run <code>mkdir -p \"fastqc_${sample_id}_logs\"</code> within the <code>script</code> block.  </p> <p>Looking at the FastQC command we can see this directory will be our output.  </p> <p>Exercise</p> <p>Replace <code>&lt; process outputs &gt;</code> with the appropriate output definition for the <code>FASTQC</code> process.  </p> Solution main.nf<pre><code>output:\npath \"fastqc_${sample_id}_logs\"\n</code></pre> <p>We've used the <code>path</code> qualifier as our output is a directory. Output  from the bash script is defined by the fastqc <code>--outdir</code> flag. </p>"},{"location":"part2/02_fastqc/#3-define-the-process-input","title":"3. Define the process <code>input</code>","text":"<p>Now we need to define the <code>input</code> block for this process. In this process,  we're going to use a combination of Nextflow operators and Groovy to do this. </p> <p>There are three inputs for this process definition that can be taken from the script definition you just added:</p> <ol> <li><code>$sample_id</code></li> <li><code>$reads_1</code></li> <li><code>$reads_2</code></li> </ol> <p>We will use a tuple as the input qualifier as it's useful to group related inputs, or, inputs that need to be processed together such as in this case.  </p> <p>We need to group these inputs together so they can be processed as a single unit. This is a requirement when working with multiple pieces of data that are specific to a sample.</p> <p>Importance of proper data grouping when using Nextflow</p> <p>Nextflow uses channels to run processes in parallel and if you aren't careful about how handle multiple pieces of data that need to be tied together, you may mix datasets up.</p> <p>We can use the input qualifier <code>tuple</code> to group multiple values into a single input definition.</p> <p>Using a tuple as the input qualifier allows us to group related inputs together.  This is useful when we need to process several pieces of data at the same time,  like in this case where we have <code>$sample_id</code> and its two read files <code>$reads_1</code>  and <code>$reads_2</code>. </p> <p></p> <p>The tuple ensures that these inputs stay linked and are processed together, preventing sample-specific data and files from getting mixed up between samples.</p> <p>In the <code>FASTQC</code> process, replace <code>&lt; process inputs &gt;</code> with the input definition:  </p> <pre><code>tuple val(sample_id), path(reads_1), path(reads_2)\n</code></pre> <ul> <li><code>val(sample_id)</code> represents the value that refers to the sample name.</li> <li><code>path(reads_1)</code> represents the path to the first read file of paired-end sequencing data.</li> <li><code>path(reads_2)</code> represent the path to the second read file of paired-end sequencing data.</li> </ul> main.nf<pre><code>process FASTQC {\n  container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  tuple val(sample_id), path(reads_1), path(reads_2)\n\n  output:\n  path \"fastqc_${sample_id}_logs\"\n\n  script:\n  \"\"\"\n  fastqc --outdir \"fastqc_${sample_id}_logs\" --format fastq $reads_1 $reads_2\n  \"\"\"\n}\n</code></pre>"},{"location":"part2/02_fastqc/#222-reading-files-with-a-samplesheet","title":"2.2.2 Reading files with a samplesheet","text":"<p>A samplesheet is a delimited text file where each row contains information or metadata that needs to be processed together. </p> Tip: using samplesheets in scalable bioinformatics workflows <p>Working with samplesheets is particularly useful when you have a combination of files and metadata that need to be assigned to a sample in a flexible manner. Typically, samplesheets are written in comma-separated (<code>.csv</code>) or tab-separated (<code>.tsv</code>) formats. </p> <p>We recommend using comma-separated files as they are less error prone and easier to read and write.</p> <p>Let's inspect <code>data/samplesheet.csv</code> with VSCode.</p> Output<pre><code>sample,fastq_1,fastq_2\ngut,data/ggal/gut_1.fq,data/ggal/gut_2.fq\n</code></pre> <p>At this stage, we are developing and testing the pipeline. As such, we're only working with one sample. The samplesheet has three columns:  </p> <ul> <li><code>sample</code>: indicates the sample name/prefix</li> <li><code>fastq_1</code>, <code>fastq_2</code>: contains the relative paths to the reads  </li> </ul> <p>The goal in this step is to read the contents of the samplesheet, and transform it so it fits the input definition of <code>FASTQC</code> we just defined:</p> <pre><code>tuple val(sample_id), path(reads_1), path(reads_2)\n</code></pre> <p>Before that, we need to add an input parameter that points to the samplesheet, called <code>input</code>.  </p> <p>Exercise</p> <p>In your <code>main.nf</code> add an input parameter called <code>reads</code> and assign the path to the samplesheet using <code>$projectDir</code>.</p> Solution main.nf<pre><code>// pipeline input parameters\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.reads = \"$projectDir/data/samplesheet.csv\"\n</code></pre> <p>In the next few steps, we will add a mix of Nextflow operators and Groovy syntax to read in and parse the samplesheet so it is in the correct format for the process we just added.  </p> <p>Using samplesheets with Nextflow can be tricky business</p> <p>There are currently no Nextflow operators specifically designed to handle samplesheets. As such, we Nextflow workflow developers have to write custom parsing logic to read and split the data. This adds complexity to our workflow development, especially when trying to handle tasks like parallel processing of samples or filtering data by sample type.</p> <p>Add the following to your workflow scope below where <code>INDEX</code> is called:</p> main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n        .view()\n\n}\n</code></pre> <p>We won't explore the logic of constructing our samplesheet input channel in depth in this lesson. The key takeaway here is to understand that using samplesheets is best practice for reading grouped files and metadata into Nextflow, and that operators and groovy needs to be chained together to get these in the correct format. The best way to do this is using a combination of Groovy and Nextflow operators.</p> <p>Our samplesheet input channel has used common Nextflow operators:</p> <ul> <li><code>.fromPath</code> creates a channel from one or more files matching a given path or pattern (to our <code>.csv</code> file, provided with the <code>--reads</code> parameter).</li> <li><code>splitCsv</code> splits the input file into rows, treating it as a CSV (Comma-Separated Values) file. The <code>header: true</code> option means that the first row of the CSV contains column headers, which will be used to access the values by name.</li> <li><code>map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }</code> uses some Groovy syntax to transform each row of the CSV file into a list, extracting the sample value, <code>fastq_1</code> and <code>fastq_2</code> file paths from the row.</li> <li><code>.view()</code> is a debugging step that outputs the transformed data to the console so we can see how the channel is structured. Its a great tool to use when building your channels.</li> </ul> Tip: using the <code>view()</code> operator for testing <p>The <code>view()</code> operator is a useful tool for debugging Nextflow workflows. It allows you to inspect the data structure of a channel at any point in the workflow, helping you to understand how the data is being processed and transformed.</p> <p>Run the workflow with the <code>-resume</code> flag:</p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>Launching `main.nf` [crazy_einstein] DSL2 - revision: 0ae3776a5e\n\n[de/fef8c4] INDEX [100%] 1 of 1, cached: 1 \u2714\n[gut, /home/setup2/hello-nextflow/part2/data/ggal/gut_1.fq, /home/setup2/hello-nextflow/part2/data/ggal/gut_2.fq]\n</code></pre> Tip: using the <code>-resume</code> flag <p>The <code>-resume</code> flag is used to resume a Nextflow workflow from where it left off. If a workflow fails or is interrupted, this flag allows you to skip tasks that were successfully completed, saving time and computational resources. It is also useful when you are developing a workflow and want to test changes without running the entire workflow from the start. </p> <p>The chain of commands produces a tuple with three elements that correspond to the row in the samplesheet. It now fits the requirements of the input definition of <code>tuple val(sample_id), path(reads_1), path(reads_2)</code>: </p> <pre><code>[gut, /home/setup2/hello-nextflow/part2/data/ggal/gut_1.fq, /home/setup2/hello-nextflow/part2/data/ggal/gut_2.fq]\n</code></pre> <p>How's it going?</p> <p>Once you have run the workflow, select the  \"Yes\" react on Zoom.</p> <p>Next, we need to assign the channel we create to a variable so it can be passed to the <code>FASTQC</code> process. Assign to a variable called <code>reads_in</code>, and remove the <code>.view()</code> operator as we now know what the output looks like.</p> main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n}\n</code></pre> <p>Now that we have an input channel with that provides the correct format ready, we will now call the <code>FASTQC</code> process.  </p> <p>Exercise</p> <p>In the <code>workflow</code> scope after where <code>reads_in</code> was defined, call the <code>FASTQC</code> process with <code>reads_in</code> as the input.</p> Solution main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in) \n    \n}\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Your output should look something like:  </p> <pre><code>Launching `main.nf` [tiny_aryabhata] DSL2 - revision: 9a45f4957b\n\nexecutor &gt;  local (1)\n[de/fef8c4] INDEX      [100%] 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1) [100%] 1 of 1 \u2714\n</code></pre> <p>If you inspect <code>results/fastqc_gut_logs</code> there is an <code>.html</code> and <code>.zip</code> file for each of the <code>.fastq</code> files.  </p> <p>Advanced exercise</p> <p>This advanced exercise walks through inspecing the output of the intermediate operators in the <code>reads_in</code> channel:  </p> <ul> <li><code>Channel.fromPath</code></li> <li><code>.splitCsv</code></li> </ul> <p>The current workflow block should look like:</p> main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n}\n</code></pre> <p><code>Channel.fromPath</code> </p> <ol> <li>In the workflow scope, comment out the lines for <code>.splitCsv</code>, <code>.map</code>, and <code>FASTQC()</code></li> <li>Add <code>.view()</code> on the line after <code>Channel.fromPath</code> and before the commented <code>.splitCsv</code></li> <li>Run the workflow with <code>-resume</code> </li> </ol> Solution main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .view()\n        //.splitCsv(header: true)\n        //.map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    //FASTQC(reads_in)\n\n}\n</code></pre> <p>The output of the <code>Channel.fromPath(params.reads)</code> step produces a path to the samplesheet:  </p> Output<pre><code>Launching `main.nf` [hungry_lalande] DSL2 - revision: 587b5b70d1\n\n[de/fef8c4] INDEX [100%] 1 of 1, cached: 1 \u2714\n/home/user1/part2/data/samplesheet.csv\n</code></pre> <p><code>.splitCsv</code> </p> <ol> <li>In the workflow scope, uncomment the line for <code>.splitCsv</code></li> <li>Move <code>.view()</code> to the line after <code>.splitCsv</code> (before the commented <code>.map</code> line)</li> <li>Run the workflow with <code>-resume</code> </li> </ol> Solution main.nf<pre><code>// Define the workflow  \nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .view()\n        //.map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    //FASTQC(reads_in)\n\n}\n</code></pre> <p><code>.splitCsv</code> takes the path from <code>.fromPath</code> and reads the header and data line as a tuple. Each element in the tuple is named by the header in the correspoding column in the samplesheet:</p> Output<pre><code>Launching `main.nf` [tiny_yonath] DSL2 - revision: 22c2c9d28f\n[de/fef8c4] INDEX | 1 of 1, cached: 1 \u2714\n[sample:gut, fastq_1:data/ggal/gut_1.fq, fastq_2:data/ggal/gut_2.fq]\n</code></pre> <p>The <code>.map</code> step takes that tuple and formats it into the tuple that is emitted by <code>reads_in</code>.</p> <p>Before proceeding, ensure to uncomment the <code>.map</code> and <code>FASTQC</code> lines, and remove <code>.view()</code>.</p> <p>Summary</p> <p>In this lesson you have learned:</p> <ol> <li>How to implement a process with a tuple input</li> <li>How to construct an input channel using operators and Groovy</li> <li>How to use the <code>.view()</code> operator to inspect the structure of a channel</li> <li>How to use the <code>-resume</code> flag to skip sucessful tasks</li> <li>How to use a samplesheet to read in grouped samples and metadata</li> </ol>"},{"location":"part2/03_quant/","title":"2.3 Multiple inputs into a single process","text":"<p>Learning objectives</p> <ol> <li>Implement a process with multiple input channels. </li> <li>Understand the importance of creating channels from process outputs.</li> <li>Implement chained Nextflow processes with channels.  </li> </ol> <p>In this lesson we will transform the bash script <code>02_quant.sh</code> into a process called <code>QUANTIFICATION</code>. This step focuses on the next phase of RNAseq data processing: quantifying the expression of transcripts relative to the reference transcriptome. </p> <p>To do this, we will need to run Salmon's quant mode over the paired-end reads and the transcriptome index.  </p> <p>Open the bash script <code>02_quant.sh</code>.  </p> 02_quant.sh<pre><code>SAMPLE_ID=gut\nREADS_1=\"data/ggal/${SAMPLE_ID}_1.fq\"\nREADS_2=\"data/ggal/${SAMPLE_ID}_2.fq\"\n\nsalmon quant \\\n    --libType=U \\\n    -i results/salmon_index \\\n    -1 ${READS_1} \\\n    -2 ${READS_2} \\\n    -o results/${SAMPLE_ID}\n</code></pre> <p>Same as the previous lesson, this script contains the <code>${SAMPLE_ID}</code> variable defintions which is used to connect sample names to fastq file paths. Within the <code>salmon quant</code> execution command: </p> <ul> <li><code>--libType=U</code> is a required argument for Salmon. </li> <li><code>-i results/salmon_index</code> is the directory output by the <code>INDEX</code> process. </li> <li><code>-1</code> and <code>-2</code> are flags for the respective paired reads (<code>.fq</code>). </li> <li><code>-o</code> outputs files into a directory called <code>results/gut</code></li> </ul>"},{"location":"part2/03_quant/#231-building-the-process","title":"2.3.1 Building the process","text":""},{"location":"part2/03_quant/#1-process-directives","title":"1. Process directives","text":"<p>Here is the empty <code>process</code> template with the <code>container</code> and <code>publishDir</code> directives we'll be using to get you started. Add this to your <code>main.nf</code> after where you defined the <code>FASTQC</code> process.  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n    &lt; script to be executed &gt;\n  \"\"\"\n}\n</code></pre> <p>It contains: </p> <ul> <li>Prefilled process directives <code>container</code> and <code>publishDir</code>.</li> <li>The empty <code>input:</code> block for us to define the input data for the process. </li> <li>The empty <code>output:</code> block for us to define the output data for the process.</li> <li>The empty <code>script:</code> block for us to define the script for the process.</li> </ul>"},{"location":"part2/03_quant/#2-define-the-process-script","title":"2. Define the process <code>script</code>","text":"<p>Update the <code>script</code> definition with the Salmon command from the bash script:</p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>The <code>--libType=U</code> is a required argument and can be left as is for the script definition. It can stay the same as in the bash script. The following need to be adjusted for the <code>QUANT</code> process: </p> <ul> <li><code>-i results/salmon_index</code> is the directory output by the <code>INDEX</code> process. </li> <li><code>-1 $reads_1</code> and <code>-2 $reads_2</code> are fastq files from the previously defined <code>reads_ch</code> channel. </li> <li><code>-o</code> outputs files into a directory named after the <code>$sample_id</code>.</li> </ul>"},{"location":"part2/03_quant/#3-define-the-process-output","title":"3. Define the process <code>output</code>","text":"<p>The <code>output</code> is a directory of <code>$sample_id</code>. In this case, it will be a directory called <code>gut/</code>. Replace <code>&lt; process outputs &gt;</code> with the following:  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n    &lt; process inputs &gt;\n\n  output:\n  path \"$sample_id\"\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre>"},{"location":"part2/03_quant/#4-define-the-process-input","title":"4. Define the process <code>input</code>","text":"<p>In this step we will define the process inputs. Based on the bash script, we have four inputs:  </p> <ul> <li><code>$salmon_index</code></li> <li><code>$sample_id</code></li> <li><code>$reads_1</code></li> <li><code>$reads_2</code></li> </ul> <p>These should look familiar! </p> <p>The <code>$salmon_index</code> was output by the <code>INDEX</code> process, and <code>$sample_id</code>, <code>$reads_1</code>, <code>$reads_2</code> are output by our <code>reads_in</code>. We will see how to chain these when we work on the <code>workflow</code> scope below.</p> <p>First, add the input definition for <code>$salmon_index</code>. Recall that we use the <code>path</code> qualifier as it is a directory:  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path salmon_index\n\n  output:\n  path \"$sample_id\"\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>Secondly, add the tuple input:  </p> main.nf<pre><code>process QUANTIFICATION {\n  container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path salmon_index\n  tuple val(sample_id), path(reads_1), path(reads_2)\n\n  output:\n  path \"$sample_id\"\n\n  script:\n  \"\"\"\n  salmon quant --libType=U -i $salmon_index -1 $reads_1 -2 $reads_2 -o $sample_id\n  \"\"\"\n}\n</code></pre> <p>Matching process inputs</p> <p>Recall that the number of inputs in the process input block and the workflow must match!</p> <p>If you have multiple inputs they need to be listed across multiple lines in the input block and listed inside the brackets in the workflow block.</p> <p>You have just defined a process with multiple inputs!  </p> <p>How's it going?</p> <p>Once you have defined the <code>process</code> block, select the  \"Yes\" react on Zoom.</p>"},{"location":"part2/03_quant/#5-call-the-process-in-the-workflow-scope","title":"5. Call the process in the <code>workflow</code> scope","text":"<p>Recall that the inputs for the <code>QUANTIFICATION</code> process are emitted by the <code>reads_in</code> channel and the output of the <code>INDEX</code> process. The <code>reads_in</code> channel  is ready to be called by the <code>QUANTIFICATION</code> process. Similarly, we need to prepare a channel for the index files output by the <code>INDEX</code> process.</p> <p>Add the following channel to your <code>main.nf</code> file, after the <code>FASTQC</code> process:</p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n}\n</code></pre> <p>Accessing process outputs</p> <p>Nextflow allows us to explicitly define the output of a channel using the <code>.out</code> attribute. If a process has 2 or more output channels, you can access them by indexing the <code>.out</code> attribute. For example: <code>.out[0]</code> for the first output, <code>.out[1]</code> for the second output.</p> <p>Alternatively, the process output definition allows the use of the <code>emit</code> statement to define a named identifier that can be used to reference the channel in the external scope.</p> <p>Call the <code>QUANTIFICATION</code> process in the workflow scope and add the inputs by adding the following line to your <code>main.nf</code> file after your <code>transcriptome_index_in</code> channel definition:  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n}\n</code></pre> <p>By doing this, we have passed two arguments to the <code>QUANTIFICATION</code> process as there are two inputs in the <code>process</code> definition. </p> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Your output should look like:  </p> Output<pre><code>Launching `main.nf` [shrivelled_cuvier] DSL2 - revision: 4781bf6c41\n\nexecutor &gt;  local (1)\n[de/fef8c4] INDEX              | 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1)         | 1 of 1, cached: 1 \u2714\n[a9/000f36] QUANTIFICATION (1) | 1 of 1 \u2714\n</code></pre> <p>A new <code>QUANTIFICATION</code> task has been successfully run and have a <code>results/gut</code> folder, with an assortment of files and directories. </p> <p>Summary</p> <p>In this lesson you have learned:  </p> <ol> <li>How to define a process with multiple input channels</li> <li>How to access a process output with <code>.out</code></li> <li>How to create a channel from a process output</li> <li>How to chain Nextflow processes with channels  </li> </ol>"},{"location":"part2/04_multiqc/","title":"2.4 Combining channels and multiple process outputs","text":"<p>Learning objectives</p> <ol> <li>Implement a channel that combines the contents of two channels.  </li> <li>Implement a process with multiple output files.  </li> </ol> <p>In this step we will transform the <code>03_multiqc.sh</code> into a process called <code>MULTIQC</code>.  This step focuses on the final step of our RNAseq data processing workflow: generating a report that summarises the quality control and quantification steps. </p> <p>To do this, we will run MultiQC, which is a popular tool  for summarising the outputs of many different bioinformatics tools. It aggregates results from all our analyses and renders it into a nice report. </p> <p>From the MultiQC docs</p> <p>MultiQC doesn\u2019t do any analysis for you - it just finds results from other tools that you have already run and generates nice reports. See here for a list of supported tools. You  can also see an example report here.</p> <p> </p> <p>Open the bash script <code>03_multiqc.sh</code>.  </p> 03_multiqc.sh<pre><code>multiqc --outdir results/ results/\n</code></pre> <p>This script is a lot simpler than previous scripts we've worked with. It searches searches for the output files generated by the <code>FASTQC</code> and <code>QUANTIFICATION</code> processes saved to the <code>results/</code> directory. As specified by <code>--outdir results/</code>, it will output two MultiQC files:  </p> <ol> <li>A directory called <code>multiqc_data/</code> </li> <li>A report file called <code>multiqc_report.html</code> </li> </ol>"},{"location":"part2/04_multiqc/#241-building-the-process","title":"2.4.1 Building the process","text":""},{"location":"part2/04_multiqc/#1-process-directives-script-and-input","title":"1. Process directives, <code>script</code>, and <code>input</code>","text":"<p>Here is the <code>process</code> template with the <code>container</code> and <code>publishDir</code> directives provided. Add this to your <code>main.nf</code> after the <code>QUANTIFICATION</code> process:  </p> main.nf<pre><code>process MULTIQC {\n  container \"quay.io/biocontainers/multiqc:1.19--pyhdfd78af_0\"\n  publishDir \"results\", mode: 'copy'\n\n  input:\n  path \"*\"\n\n  output:\n    &lt; process outputs &gt;\n\n  script:\n  \"\"\"\n  multiqc .\n  \"\"\"\n}\n</code></pre> <p>The <code>script</code> and <code>input</code> follow the MultiQC Nextflow integration recommendations.  The key thing to note here is that MultiQC needs to be run once for all upstream outputs. </p> <p>From the information above we know that the input for <code>multiqc</code> is the  <code>results/</code> directory, specifically, the files and directories within <code>results/</code>. We will need to bring the outputs of the <code>FASTQC</code> (<code>fastqc_gut_logs/</code>) and <code>QUANTIFICATION</code> (<code>gut/</code>) processes into a single channel as input to <code>MULTIQC</code>.  </p> <p>Why you should NOT use the <code>publishDir</code> folder as a process input</p> <p>It might make sense to have the <code>results/</code> folder (set by <code>publishDir</code>) as the input to the process here, but it may not exist until the workflow finishes. </p> <p>Using the <code>publishDir</code> as a process input can cause downstream processes  prematurely, even if the directory is empty or incomplete. In this case,  MultiQC might miss some inputs.</p> <p>Use channels to pass data between processes. Channels enable Nextflow to track outputs and ensure that downstream processes only run when all required data is ready, maintaining proper worfklow control.</p> <p>More on this in the next section.  </p>"},{"location":"part2/04_multiqc/#2-define-the-process-output","title":"2. Define the process <code>output</code>","text":"<p>The MultiQC output consists of the following:</p> <ul> <li>An HTML report file called <code>multiqc_report.html</code></li> <li>A directory called <code>multiqc_data/</code> containing the data used to generate the report.</li> </ul> <p>Add the following <code>output</code> definition to the <code>MULTIQC</code> process:  </p> main.nf<pre><code>process MULTIQC {\n\n  container \"quay.io/biocontainers/multiqc:1.19--pyhdfd78af_0\"\n  publishDir params.outdir, mode: 'copy'\n\n  input:\n  path \"*\"  \n\n  output:\n  path \"multiqc_report.html\"\n  path \"multiqc_data\"\n\n  script:\n  \"\"\"\n  multiqc .\n  \"\"\"\n}\n</code></pre>"},{"location":"part2/04_multiqc/#242-combining-channels-with-operators","title":"2.4.2 Combining channels with operators","text":"<p>Tip</p> <p>When running MultiQC, it needs to be run once on all the upstream input files. This is so a single report is generated with all the results.</p> <p>In this case, the input files for the <code>MULTIQC</code> process are outputs from <code>FASTQC</code> and <code>QUANTIFICATION</code> processes. Both FastQC and Salmon are supported by MultiQC and the required files are detected automatically by the program (when using it a Nextflow pipeline, there is some pre-processing that needs to be done).</p> <p>The goal of this step is to bring the outputs from <code>MULTIQC</code> and <code>QUANTIFICATION</code> processes into a single input channel for the <code>MULTIQC</code> process. This ensures that MultiQC is run once.  </p> <p>The next few additions will involve chaining together Nextflow operators to correctly format inputs for the <code>MULTIQC</code> process.  </p> <p>Poll</p> <p>What Nextflow input type (qualifier) ensures that inputs are grouped and processed together?</p> <p>Add the following to the workflow block in your <code>main.nf</code> file, under the <code>QUANTIFICATION</code> process.  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    FASTQC.out[0]\n        .mix(QUANTIFICATION.out[0])\n        .view()\n\n}\n</code></pre> <p>This channel creates a tuple with the two inputs as elements:</p> <ul> <li>Takes the output of <code>FASTQC</code>, using element <code>[0]</code> to refer to the first element of the output. </li> <li>Uses <code>mix(QUANTIFICATION.out[0])</code> to combine <code>FASTQC.out[0]</code> output with the first element of the <code>QUANTIFICATION</code> output.</li> <li>Uses <code>view()</code> allows us to see the values emitted into the channel.</li> </ul> <p>For more information, see the documentation on <code>mix</code>.</p> <p>Run the workflow to see what it produces:  </p> <pre><code>nextflow run main.nf -resume  \n</code></pre> <p>The output should look something like:  </p> Output<pre><code>Launching `main.nf` [stupefied_minsky] DSL2 - revision: 82245ce02b\n\n[de/fef8c4] INDEX              | 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1)         | 1 of 1, cached: 1 \u2714\n[a9/000f36] QUANTIFICATION (1) | 1 of 1, cached: 1 \u2714\n/home/user1/part2/work/bb/32a3aaa5e5fd68265f0f34df1c87a5/fastqc_gut_logs\n/home/user1/part2/work/a9/000f3673536d98c8227b393a641871/gut\n</code></pre> <p>The outputs have been emitted one after the other, meaning that it will be processed separately. We need them to be processed together (generated in the same MultiQC report), so we need to add one more step.  </p> <p>Note</p> <p>Note that the outputs point to the files in the work directories, rather than the <code>publishDir</code>. This is one of the ways that Nextflow ensures all input files are ready and ensures proper workflow control.</p> <p>Add the <code>collect</code> operator to ensure all samples are processed together in the same process and view the output:  </p> main.nf<pre><code>// Define the workflow\nworkflow {\n\n    // Run the index step with the transcriptome parameter\n    INDEX(params.transcriptome_file)\n\n    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n\n    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    FASTQC.out[0]\n        .mix(QUANTIFICATION[0])\n        .collect()\n        .view()\n\n}\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume  \n</code></pre> <p>The channel now outputs a single tuple with the two directories:  </p> Output<pre><code>Launching `main.nf` [small_austin] DSL2 - revision: 6ab927f137\n\n[de/fef8c4] INDEX              | 1 of 1, cached: 1 \u2714\n[bb/32a3aa] FASTQC (1)         | 1 of 1, cached: 1 \u2714\n[a9/000f36] QUANTIFICATION (1) | 1 of 1, cached: 1 \u2714\n[/home/user1/part2/work/bb/32a3aaa5e5fd68265f0f34df1c87a5/fastqc_gut_logs, /home/user1/part2/work/a9/000f3673536d98c8227b393a641871/gut]\n</code></pre> <p>Now that we have a channel that emits the correct data, add the finishing touches to the workflow scope.</p> <p>Exercise: Assign the input channel</p> <ol> <li>Assign the chain of operations to a channel called <code>multiqc_in</code></li> <li>Remove the <code>.view()</code> operator  </li> </ol> Solution main.nf<pre><code>    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    multiqc_in = FASTQC.out[0]\n        .mix(QUANTIFICATION[0])\n        .collect()\n\n}\n</code></pre> <p>Exercise: call the <code>MULTIQC</code> process</p> <ol> <li>Add the <code>MULTIQC</code> process in the workflow scope</li> <li>Pass the <code>multiqc_in</code> channel as input.</li> </ol> Solution main.nf<pre><code>    // Define the quantification channel for the index files\n    transcriptome_index_in = INDEX.out[0]\n\n    // Run the quantification step with the index and reads_in channels\n    QUANTIFICATION(transcriptome_index_in, reads_in)\n\n    // Define the multiqc input channel\n    multiqc_in = FASTQC.out[0]\n        .mix(QUANTIFICATION[0])\n        .collect()\n\n    // Run the multiqc step with the multiqc_in channel\n     MULTIQC(multiqc_in)\n\n}\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume  \n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>Launching `main.nf` [hopeful_swanson] DSL2 - revision: a4304bbe73\n\n[aa/3b8821] INDEX          [100%] 1 of 1, cached: 1 \u2714\n[c2/baa069] QUANTIFICATION [100%] 1 of 1, cached: 1 \u2714\n[ad/e49b20] FASTQC         [100%] 1 of 1, cached: 1 \u2714\n[a3/1f885c] MULTIQC        [100%] 1 of 1 \u2714\n</code></pre>"},{"location":"part2/04_multiqc/#243-inspecting-the-multiqc-report","title":"2.4.3 Inspecting the MultiQC report","text":"<p>Let's inspect the generated MultiQC report. You will need to download the file to your local machine and open it in a web browser.  </p> <p>Exercise</p> <ol> <li>In the VSCode Explorer sidebar, locate the report <code>results/multiqc_report.html</code> </li> <li>Right click on the file, and select \"Download\"</li> <li>Open the file in a web browser</li> </ol> <p>Poll</p> <p>Under the \"General Statistics\" section, how many samples (i.e. rows) have been included in the table?</p> <p>Tip</p> <p>If you have to view many (i.e. <code>.html</code>) files on a remote server, we recommend using the  Live Server VSCode extension. </p> <p>The extension allows you to view <code>.html</code> files within a VSCode tab instead of manually downloading files locally.</p> <p>You have a working pipeline for a single paired-end sample!</p> <p>Summary</p> <p>In this lesson you have learned:</p> <ol> <li>How to implement a process following integration recommendations</li> <li>How to define an output with multiple outputs</li> <li>How to use the <code>mix</code> and <code>collect</code> operators to combine outputs into a single tuple</li> <li>How to access and view <code>.html</code> files from a remote server</li> </ol>"},{"location":"part2/05_scale/","title":"2.5 Productionising our workflow","text":"<p>Learning objectives</p> <ol> <li>Configure Nextflow workflows to run on multiple samples</li> <li>Enable and interpret Nextflow's inbuilt reports </li> <li>Implement the <code>tag</code> directive to label tasks for better tracking and profiling</li> <li>Configure a Nextflow workflow to use multiple CPUs for a process </li> </ol> <p>Now that we have a working pipeline on a single-sample, we will update it  to take multiple samples and introduce Nextflow concepts that not only help with understanding and profiling the pipeline but also set the stage for productionising it. </p> <p>We will focus on making the workflow scalable, robust, and efficient for real-world data processing. Key productionisation practices include: </p> <ul> <li>Automating tasks</li> <li>Handling errors gracefully</li> <li>Optimising resource usage</li> <li>Ensuring reproducibility. </li> </ul> <p>These steps ensure that the pipeline can be reliably used in more complex scenarios, like when processing multiple samples in parallel. </p>"},{"location":"part2/05_scale/#251-labeling-tasks-with-the-tag-directive","title":"2.5.1 Labeling tasks with the <code>tag</code> directive","text":"<p>The tag process directive allows you to add a custom label, or tag, to each task that gets executed. It is useful for identifying what is being run when the workflow is being executed in a bit more detail. It is especially helpful showing you  what is being run when we run multiple samples, and for profiling later.</p> <p>Add the following <code>tag</code> directives to your existing <code>FASTQC</code> and <code>QUANTIFICATION</code> processes. </p> <p>For <code>FASTQC</code>:</p> main.nf<pre><code>process FASTQC {\n    tag \"fastqc on ${sample_id}\"\n    container \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n    publishDir \"results\", mode: 'copy'\n</code></pre> <p>And for <code>QUANTIFICATION</code>:  </p> main.nf<pre><code>process QUANTIFICATION {\n    tag \"salmon on ${sample_id}\"\n    container \"quay.io/biocontainers/salmon:1.10.1--h7e5ed60_0\"\n    publishDir \"results\", mode: 'copy'\n</code></pre> <p>The tags we just added indicates what program is being run (<code>fastqc</code> or  <code>salmon</code>), and on which sample (<code>${sample_id}</code>) it is being run on. </p> <p>Run the pipeline with the updated tags:  </p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>The output should look similar to:  </p> Output<pre><code>Launching `main.nf` [distraught_bell] DSL2 - revision: dcb06191e7\n\nexecutor &gt;  local (5)\n[aa/3b8821] INDEX                           | 1 of 1, cached: 1 \u2714\n[c2/baa069] FASTQC (fastqc on gut)          | 1 of 1, cached: 1 \u2714\n[ad/e49b20] QUANTIFICATION (salmon on gut)  | 1 of 1, cached: 1 \u2714\n[a3/1f885c] MULTIQC                         | 1 of 1, cached: 1 \u2714\n</code></pre> <p>No new tasks were run, but <code>FASTQC</code> and <code>QUANTIFICATION</code> processes now have labels appended in the execution output.  </p>"},{"location":"part2/05_scale/#252-using-a-samplesheet-with-multiple-samples","title":"2.5.2 Using a samplesheet with multiple samples","text":"<p>Recall that the samplesheet is used to control which files/data are analysed by the workflow. Inspect <code>data/samplesheet_full.csv</code>.  </p> samplesheet_full.csv<pre><code>sample,fastq_1,fastq_2\ngut,data/ggal/gut_1.fq,data/ggal/gut_2.fq\nliver,data/ggal/liver_1.fq,data/ggal/liver_2.fq\nlung,data/ggal/lung_1.fq,data/ggal/lung_2.fq\n</code></pre> <p>Compared to the samplesheet we have been using <code>data/samplesheet.csv</code>, this one contains two additional lines for the <code>liver</code> and <code>lung</code> paired reads.</p> <p>Next we will run the workflow with all three samples by overwriting the default  input for <code>reads</code> with <code>data/samplesheet_full.csv</code> using the double hyphen  approach <code>--reads</code> in the run command.</p> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume --reads data/samplesheet_full.csv\n</code></pre> <p>Your output should look similar to:  </p> Output<pre><code>Launching `main.nf` [distraught_bell] DSL2 - revision: dcb06191e7\n\nexecutor &gt;  local (5)\n[de/fef8c4] INDEX                           | 1 of 1, cached: 1 \u2714\n[4e/b4c797] FASTQC (fastqc on liver)        | 3 of 3, cached: 1 \u2714\n[36/93c8b4] QUANTIFICATION (salmon on lung) | 3 of 3, cached: 1 \u2714\n[e7/5d91ea] MULTIQC                         | 1 of 1 \u2714\n</code></pre> <p>There are two new tasks run for <code>FASTQC</code> and <code>QUANTIFICATION</code>. Our newly added tags indicate which samples they were run on - either <code>lung</code> or <code>liver</code> reads!</p> <p>Note</p> <p>Updating the <code>params.reads</code> definition in your <code>main.nf</code> script can save having to add the <code>--reads</code> flag every time you want to run it with a different samplesheet.</p> <p>Advanced Exercise</p> <ol> <li>Update the workflow scope to inspect the output of the <code>reads_in</code> channel (i.e. with <code>.view()</code>)</li> <li>Run the workflow with <code>samplesheet_full.csv</code></li> </ol> <p>What has changed with what the <code>reads_in</code> channel is emitting?</p> Solution <p>Viewing <code>reads_in</code>: </p> main.nf<pre><code>    // Define the fastqc input channel\n    reads_in = Channel.fromPath(params.reads)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, file(row.fastq_1), file(row.fastq_2)] }\n\n    reads_in.view()\n\n    // Run the fastqc step with the reads_in channel\n    FASTQC(reads_in)\n</code></pre> <p>Run the workflow:  </p> <pre><code>nextflow run main.nf -resume --reads data/samplesheet_full.csv\n</code></pre> <p>Your output should look something like:  </p> Output<pre><code>executor &gt;  local (5)\n[de/fef8c4] INDEX                           | 1 of 1, cached: 1 \u2714\n[4e/b4c797] FASTQC (fastqc on liver)        | 3 of 3, cached: 3 \u2714\n[36/93c8b4] QUANTIFICATION (salmon on lung) | 3 of 3, cached: 3 \u2714\n[e7/5d91ea] MULTIQC                         | 1 of 1 \u2714\n[gut, .../data/ggal/gut_1.fq, .../data/ggal/gut_2.fq]\n[liver, .../data/ggal/liver_1.fq, .../data/ggal/liver_2.fq]\n[lung, .../data/ggal/lung_1.fq, .../data/ggal/lung_2.fq]\n</code></pre> <p>There are now a total of three tuples emitted separately for each sample.  When passed into <code>FASTQC</code> and <code>QUANTIFICATION</code>, each tuple is processed separately in independent tasks.</p> <p>Remove <code>reads_in.view()</code> before proceeding.</p>"},{"location":"part2/05_scale/#253-an-introduction-to-configuration","title":"2.5.3 An introduction to configuration","text":"<p>In this section, we will explore how Nextflow workflows can be configured to utilise the computational resources available. Whilst there are many ways to configure Nextflow workflows (especially on HPC clusters), we will focus on increasing the number of CPUs used to speed up tasks.  </p> <p>Some bioinformatics tool, like FastQC, support multithreading to speed up analyses. From the <code>fastqc --help</code> command, you'll notice the following option:</p> Output<pre><code>-t --threads    Specifies the number of files which can be processed    \n                simultaneously.\n</code></pre> <p>This means we can configure the number of threads (or CPUs) that FastQC uses to process multiple files in parallel to speed up the analysis. In Nextflow, we control this through the  <code>cpus</code> directive.</p> <p>Recall that our <code>FASTQC</code> takes as input the <code>reads_in</code> channel which emits two <code>.fastq</code> files. We will configure the process to use 2 CPUs so each file gets run on 1 CPU each (the maximum CPUs fastqc will use per file), simulataneously.</p> <p>In your <code>main.nf</code> script, update the <code>script</code> definition in the <code>FASTQC</code> process to add the multithreading option:  </p> main.nf<pre><code>    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc --outdir \"fastqc_${sample_id}_logs\" -f fastq $reads_1 $reads_2 -t $task.cpus\n    \"\"\"\n</code></pre> <ul> <li>The <code>task.cpus</code> variable is automatically populated with the number of  CPUs allocated to the task based on the Nextflow configuration. By default this is 1.  </li> </ul> <p>Next, we need to update our <code>nextflow.config</code> file to configure the number of CPUs to be used. To allow each FastQC process to use 2 CPUs, update the config file as follows:  </p> nextflow.config<pre><code>process.cpus = 2\ndocker.enabled = true\n</code></pre> <p>The <code>-t $task.cpus</code> argument will populate as <code>-t 2</code> when we run the workflow next. Before we do, we will explore Nextflow's built-in reporting system to assess resource usage.</p>"},{"location":"part2/05_scale/#254-inspecting-workflow-performance","title":"2.5.4 Inspecting workflow performance","text":"<p>When running workflows, it is helpful to understand how each part of your workflow is using resources like CPUs, memory, and the time taken to complete. Nextflow can generate text-based and visual reports that give you clear picture of how your workflow ran and identify areas for improvement.  </p> <p>We will explore some of Nextflow's built-it in tools that can show these important details of how tasks ran.</p> <p>To enable these reports, add the following to your <code>nextflow.config</code> file:</p> nextflow.config<pre><code>process.cpus = 2\ndocker.enabled = true\n\n// enable reporting\ndag.enabled = true\nreport.enabled = true\ntimeline.enabled = true\ntrace.enabled = true\n</code></pre> <p>Run the workflow. To assess the resource usage all processes need to be run again so <code>-resume</code> should not be used. (If we resume now, it will still appear as a cached run, with limited information).  </p> <pre><code>nextflow run main.nf --reads \"data/samplesheet_full.csv\"\n</code></pre> <p>Inspect your project directory. You should have 3 <code>.html</code> files and a <code>.txt</code> file with matching timestamps. A summary of the different reports are included in the table below. For a detailed description of each report see the Nextflow documentation on reports.</p> Report type Description <code>dag</code> A high-level graph that shows how processes and channels are connected to each other. <code>report</code> A visual summary of the time and resources used grouped by process. <code>timeline</code> A Gannt chart that shows when each task started and ended. <code>trace</code> A detailed text log with the time and resources used by each task. <p>Complete the following steps in the exercise to view the report file <code>report-*.html</code> in your local browser.  </p> <p>Exercise</p> <ol> <li>In the VSCode file explorer sidebar, locate the report file (e.g. <code>report-*.html</code>)</li> <li>Right click on the file and select \"Download\" to save it to your local computer.</li> <li>Open the <code>report-*.html</code> in a browser.</li> <li>Navigate to \"Resource Usage\" -&gt; \"CPU\".</li> <li>Hover over the <code>FASTQC</code> bar chart and note the <code>mean</code> CPU usage.</li> </ol> <p>Poll</p> <p>What was the <code>mean</code> CPU usage for your <code>FASTQC</code> process?</p> Solution <p>In this report, a mean of 2.53 CPUs were utilised by the <code>FASTQC</code> process across the 3 samples. This value will slightly differ across runs.</p> <p></p> <p>You have successfully run, configured, and profiled a multi-sample workflow!</p> <p>Summary</p> <p>In this lesson you have learned:</p> <ol> <li>How to add custom labels with process tags</li> <li>How to use <code>task.cpus</code> to enable multithreading within processes</li> <li>How to configure process resources with <code>nextflow.config</code></li> <li>How to enable and view Nextflow workflow reports</li> </ol>"}]}